caravel_config_py.tmpl: |
  """The main config file for Caravel

  All configuration in this file can be overridden by providing a caravel_config
  in your PYTHONPATH as there is a ``from caravel_config import *``
  at the end of this file.
  """
  from __future__ import absolute_import
  from __future__ import division
  from __future__ import print_function
  from __future__ import unicode_literals
  from caravel import app

  import json
  import os
  import configparser

  from dateutil import tz
  from flask_appbuilder.security.manager import AUTH_DB

  BASE_DIR = os.path.abspath(os.path.dirname(__file__))
  DATA_DIR = os.path.join(os.path.expanduser('~'), '.caravel')
  if not os.path.exists(DATA_DIR):
      os.makedirs(DATA_DIR)

  try:
      # Find path of base dir of the install, use default if parsing path fails.
      LIB_PATH = "/connectors/reveal/lib"
      REVEAL_PATH = "/connectors/reveal"
      INSTALL_DIR = BASE_DIR.split(LIB_PATH)[0] + REVEAL_PATH
  except Exception:
      INSTALL_DIR = '/opt/gpudb/connectors/reveal'

  # ---------------------------------------------------------
  # Caravel specific config
  # ---------------------------------------------------------
  PACKAGE_DIR = os.path.join(BASE_DIR, 'static', 'assets')
  PACKAGE_FILE = os.path.join(PACKAGE_DIR, 'package.json')
  with open(PACKAGE_FILE) as package_file:
      VERSION_STRING = json.load(package_file)['version']

  ROW_LIMIT = 25000
  CARAVEL_WORKERS = 8

  CARAVEL_WEBSERVER_ADDRESS = '0.0.0.0'
  CARAVEL_WEBSERVER_PORT = 8088
  CARAVEL_WEBSERVER_TIMEOUT = 60

  CUSTOM_SECURITY_MANAGER = None
  # ---------------------------------------------------------

  # Custom hostname, also parsed by gpudb-reveal.sh, hardcode or leave empty
  THIS_HOSTNAME = ''

  # Your App secret key
  SECRET_KEY = '\2\1thisismyscretkey\1\2\e\y\y\h'  # noqa

  # The SQLAlchemy connection string.

  # N.B: gpudb-reveal.sh reads/manipulates CUSTOM_DB_PATH and reads CARAVEL_DB_FILE
  # and replicates some of the logic in assembling SQLALCHEMY_DATABASE_URI; please change in concert if needed
  # CUSTOM_DB_PATH and CARAVEL_DB_FILE should be hard-coded or empty, not built from variables
  CUSTOM_DB_PATH = '/opt/gpudb/persist/gpudb/reveal'
  CARAVEL_DB_FILE = 'caravel.db'
  CARAVEL_DB_TIMEOUT = 120
  if CUSTOM_DB_PATH != '':
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + CUSTOM_DB_PATH + '/' + CARAVEL_DB_FILE + '?timeout=' + str(CARAVEL_DB_TIMEOUT)
  elif THIS_HOSTNAME != '':
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + INSTALL_DIR + '/var/' + THIS_HOSTNAME + '/' + CARAVEL_DB_FILE + '?timeout=' + str(CARAVEL_DB_TIMEOUT)
  else:
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + INSTALL_DIR + '/var/' + CARAVEL_DB_FILE + '?timeout=' + str(CARAVEL_DB_TIMEOUT)
  # ---------------------------------------------------------

  # Whether to reuse engine instances across requests
  SQLALCHEMY_ENGINE_REUSE = True

  # The limit of queries fetched for query search
  QUERY_SEARCH_LIMIT = 1000

  # Flask-WTF flag for CSRF
  CSRF_ENABLED = True

  # Whether to run the web server in debug mode or not
  DEBUG = False

  # Whether to show the stacktrace on 500 error
  SHOW_STACKTRACE = False

  # Extract and use X-Forwarded-For/X-Forwarded-Proto headers?
  ENABLE_PROXY_FIX = True

  # Whether to allow anonymous access
  ENABLE_ANON_LOGIN = False
  ENABLE_AUTO_LOGIN = False
  ANON_LOGIN_BUTTON = 'Guest Login'
  ANON_LOGIN_USERNAME = ""
  ANON_LOGIN_PASSWORD = ""

  # Enable for embedding inside iframe
  # by setting SECURE to True and SAMESITE to 'None'
  # SESSION_COOKIE_HTTPONLY = True
  # SESSION_COOKIE_SECURE = True
  # SESSION_COOKIE_SAMESITE = 'None'

  # Configure login page title and subtitle
  LOGIN_TITLE = 'Reveal'
  LOGIN_SUBTITLE = 'Visual Data Exploration'

  # Whether to enable animation on login page
  ENABLE_LOGIN_ANIMATION = True

  # Whether to enable sharing/embedding of slices
  ENABLE_SLICE_EMBED = True
  EMBED_USE_CREDENTIALS = False

  # Static asset cache ID for expiration on restart
  STATIC_CACHE_ID = os.environ.get('STATIC_CACHE_ID', '0')

  # Whether database auth is enabled
  # Should be automatic based on gpudb.conf
  ENABLE_DB_AUTH = False
  try:
      REQUIRE_AUTH_KEY = "require_authentication"
      CONF_FILE = BASE_DIR.split(LIB_PATH)[0] + '/core/etc/gpudb.conf'
      conf = configparser.ConfigParser()
      conf.read(CONF_FILE)
      require = conf.getboolean('gaia', REQUIRE_AUTH_KEY)
      if require is not None and require:
          ENABLE_DB_AUTH = require
  except Exception as e:
      # No action needed already defaults to False
      pass

  # Which users should have full datasource access
  ALL_DATASOURCE_ACCESS_ROLES = ('Admin', 'Designer')

  # GPUdb host, port, and path for proxy
  # Path format: '/path'
  GPUDB_PROXY_AUTO_UPDATE = True
  GPUDB_PROXY_HOST = '127.0.0.1'
  GPUDB_PROXY_PORT = '8082'
  GPUDB_PROXY_PATH = '/gpudb-0'

  # GPUdb proxy protocol override
  # Default is to use request protocol
  GPUDB_PROXY_PROTOCOL = 'http'

  # GAdmin port for table export proxy.
  GADMIN_PORT = '8080'

  # ODBC Port that KiSQL uses
  KISQL_PORT = '9191'

  # Enable advanced Slice management
  ENABLE_SLICE_MGMT = False

  # DiskCache folder location
  DISK_CACHE_FOLDER = 'tmp'
  if THIS_HOSTNAME != '':
    DISK_CACHE_FOLDER = 'tmp/' + THIS_HOSTNAME

  # Google Analytics Tracking ID
  # GA_TRACKING_ID = ''

  # Config ID to support versioning
  CONFIG_VERSION_ID = int(round(os.path.getmtime(os.path.realpath(INSTALL_DIR + '/etc/default.json'))))

  # Default Kinetica database name
  KINETICA_DB_NAME = 'Kinetica'

  # ------------------------------
  # GLOBALS FOR APP Builder
  # ------------------------------
  # Uncomment to setup Your App name
  APP_NAME = "Kinetica Reveal"

  # Uncomment to setup Setup an App icon
  APP_ICON = "/static/assets/images/caravel_logo.png"

  # Druid query timezone
  # tz.tzutc() : Using utc timezone
  # tz.tzlocal() : Using local timezone
  # other tz can be overridden by providing a local_config
  DRUID_IS_ACTIVE = False
  DRUID_TZ = tz.tzutc()

  # ----------------------------------------------------
  # AUTHENTICATION CONFIG
  # ----------------------------------------------------
  # The authentication type
  # AUTH_OID : Is for OpenID
  # AUTH_DB : Is for database (username/password()
  # AUTH_LDAP : Is for LDAP
  # AUTH_REMOTE_USER : Is for using REMOTE_USER from web server
  AUTH_TYPE = AUTH_DB

  # Uncomment to setup Full admin role name
  # AUTH_ROLE_ADMIN = 'Admin'

  # Uncomment to setup Public role name, no authentication needed
  # AUTH_ROLE_PUBLIC = 'Public'

  # Will allow user self registration
  # AUTH_USER_REGISTRATION = True

  # Will allow user auto registration
  AUTH_USER_REGISTRATION_AUTO = os.getenv('REVEAL_AUTH_USER_REGISTRATION_AUTO') == 'True' or os.getenv('REVEAL_AUTH_USER_REGISTRATION_AUTO') == None

  # The default user self registration role
  AUTH_USER_REGISTRATION_ROLE = os.getenv('REVEAL_AUTH_USER_REGISTRATION_ROLE') or "Dashboard"

  # When using LDAP Auth, setup the ldap server
  # AUTH_LDAP_SERVER = "ldap://ldapserver.new"

  # Uncomment to setup OpenID providers example for OpenID authentication
  # OPENID_PROVIDERS = [
  #    { 'name': 'Yahoo', 'url': 'https://me.yahoo.com' },
  #    { 'name': 'AOL', 'url': 'http://openid.aol.com/<username>' },
  #    { 'name': 'Flickr', 'url': 'http://www.flickr.com/<username>' },
  #    { 'name': 'MyOpenID', 'url': 'https://www.myopenid.com' }]

  # ---------------------------------------------------
  # Roles config
  # ---------------------------------------------------
  # Grant public role the same set of permissions as for the GAMMA role.
  # This is useful if one wants to enable anonymous users to view
  # dashboards. Explicit grant on specific datasets is still required.
  PUBLIC_ROLE_LIKE_GAMMA = False

  # ---------------------------------------------------
  # Babel config for translations
  # ---------------------------------------------------
  # Setup default language
  BABEL_DEFAULT_LOCALE = 'en'
  # Your application default translation path
  BABEL_DEFAULT_FOLDER = 'babel/translations'
  # The allowed translation for you app
  LANGUAGES = {
      'en': {'flag': 'us', 'name': 'English'},
      # 'fr': {'flag': 'fr', 'name': 'French'},
      # 'zh': {'flag': 'cn', 'name': 'Chinese'},
  }
  # ---------------------------------------------------
  # Image and file configuration
  # ---------------------------------------------------
  # The file upload folder, when using models with files
  UPLOAD_FOLDER = BASE_DIR + '/app/static/uploads/'

  # The image upload folder, when using models with images
  IMG_UPLOAD_FOLDER = BASE_DIR + '/app/static/uploads/'

  # The image upload url, when using models with images
  IMG_UPLOAD_URL = '/static/uploads/'
  # Setup image size default is (300, 200, True)
  # IMG_SIZE = (300, 200, True)

  CACHE_DEFAULT_TIMEOUT = 300 # flask cache
  CACHE_CONFIG = {'CACHE_TYPE': 'simple'}

  # CORS Options
  ENABLE_CORS = False
  CORS_OPTIONS = {}


  # ---------------------------------------------------
  # List of viz_types not allowed in your environment
  # For example: Blacklist pivot table and treemap:
  #  VIZ_TYPE_BLACKLIST = ['pivot_table', 'treemap']
  # ---------------------------------------------------

  VIZ_TYPE_BLACKLIST = []

  # ---------------------------------------------------
  # List of data sources not to be refreshed in druid cluster
  # ---------------------------------------------------

  DRUID_DATA_SOURCE_BLACKLIST = []

  # --------------------------------------------------
  # Modules and datasources to be registered
  # --------------------------------------------------
  DEFAULT_MODULE_DS_MAP = {'caravel.models': ['DruidDatasource', 'SqlaTable']}
  ADDITIONAL_MODULE_DS_MAP = {}


  """
  1) http://docs.python-guide.org/en/latest/writing/logging/
  2) https://docs.python.org/2/library/logging.config.html
  """

  # Console Log Settings

  LOG_FORMAT = '%(asctime)s:%(levelname)s:%(name)s:%(message)s'
  LOG_LEVEL = 'WARNING'

  # ---------------------------------------------------
  # Enable Time Rotate Log Handler
  # ---------------------------------------------------
  # LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL

  ENABLE_TIME_ROTATE = False
  TIME_ROTATE_LOG_LEVEL = 'DEBUG'
  FILENAME = os.path.join(DATA_DIR, 'caravel.log')
  ROLLOVER = 'midnight'
  INTERVAL = 1
  BACKUP_COUNT = 30

  # Set whether to enable/disable action logging
  ENABLE_ACTION_LOGS = True

  # Set this API key to enable Mapbox visualizations
  MAPBOX_API_KEY = ""

  # Maximum number of rows returned in the SQL editor
  SQL_MAX_ROW = 1000

  # If defined, shows this text in an alert-warning box in the navbar
  # one example use case may be "STAGING" to make it clear that this is
  # not the production version of the site.
  WARNING_MSG = None

  # Default celery config is to use SQLA as a broker, in a production setting
  # you'll want to use a proper broker as specified here:
  # http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html
  CELERY_CONFIG = None
  SQL_CELERY_DB_FILE_PATH = os.path.join(DATA_DIR, 'celerydb.sqlite')
  SQL_CELERY_RESULTS_DB_FILE_PATH = os.path.join(DATA_DIR, 'celery_results.sqlite')

  # static http headers to be served by your Caravel server.
  # The following example prevents iFrame from other domains
  # and "clickjacking" as a result
  # HTTP_HEADERS = {'X-Frame-Options': 'SAMEORIGIN'}
  HTTP_HEADERS = {}

  # The db id here results in selecting this one as a default in SQL Lab
  DEFAULT_DB_ID = None

  # Timeout duration for SQL Lab synchronous queries
  SQLLAB_TIMEOUT = 30

  try:
      from caravel_config import *  # noqa
  except ImportError:
      pass

  if not CACHE_DEFAULT_TIMEOUT:
      CACHE_DEFAULT_TIMEOUT = CACHE_CONFIG.get('CACHE_DEFAULT_TIMEOUT')
caravel_default_json.tmpl: |
  {
    "autoUpdateUrls": false,
    "kinetica": {
      "api": {
        "url": "/caravel/proxy"
      },
      "wms": {
        "url": "/caravel/wms"
      },
      "gadmin": {
        "url": "http://localhost:8080/gadmin"
      },
      "auth": {
        "autocreateuser": true,
        "exemptuser": "admin"
      },
      "debounce": {
        "delay": 400
      },
      "render": {
        "delay": 30
      },
      "filter": {
        "delay": 30
      },
      "broadcast": {
        "delay": 30
      },
      "collection_name": "__REVEAL_TMP"
    },
    "mapbox": {
      "apiKey": ""
    },
    "bing": {
      "apiKey": ""
    },
    "slices": {
      "kinetica_map": {
        "popup": {
          "pagesize": 100
        }
      },
      "kinetica_data": {
        "download": {
          "rowlimit": 100000
        }
      }
    },
    "docs": {
      "expressions": {
        "url": ":8080/docs/concepts/expressions.html"
      },
      "textSearch": {
        "url": ":8080/docs/concepts/full_text_search.html"
      }
    },
    "test": {
      "success": {
        "id": "selenium",
        "class": "automated-test-id"
      }
    }
  }
filter-gpudb-fluent-bit.conf.tmpl: |-
  [FILTER]
      Name  stdout
      Match *
fluent-bit.conf.tmpl: |-
  [SERVICE]
      Flush          1
      Daemon         Off
      Log_Level      info
      #    Parsers_File   parsers.conf

  @INCLUDE input-gpudb-fluent-bit.conf
  @INCLUDE filter-gpudb-fluent-bit.conf
  @INCLUDE output-otel-fluent-bit.conf
  # @INCLUDE output-stdout-fluent-bit.conf
gadmin_settings_js.tmpl: |
  var HA_ENABLED = false; //enable the HA features for gadmin and disables features not available for HA
  var APPLICATION_NAME = "Kinetica"; // change this to gaia if you want to show gaia instead of GPUdb and viceversa
  var APPLICATION_FULL_NAME = APPLICATION_NAME  + (HA_ENABLED ? "-HA" : "");
  var COLORING_LEGEND = true; // determines whether coloring legends are used for fields depending on where the fields are stored
  var WEBAPP_CONTEXT = "gadmin";

  // URL to Kinetica proxy servlet
  // Change this if you want to access gadmin from a reverse proxy that uses a different URL
  // Ex:
  // PROXY_URL = window.location.origin + "/bobs-gadmin/ ";
  // reverse proxy points to server:8080/gadmin/
  // Browser access gadmin at www.gpudb.com/bobs-gadmin/
  var PROXY_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/proxy/";
  var PROXY_EXTERNAL_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/external/proxy/";

  // where Grafana is running (do not have slash / at the end)
  var GRAFANA_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/stats";

  var PROXY_HOSTMANAGER_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/hostmanager/proxy/";
  var PROXY_HOSTMANAGER_EXTERNAL_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/external/hostmanager/proxy/";

  //The HA gadmin configuration is intented to run as a HA cluster sytem level view.  It therefore needs to run as
  //a seperate web application. Each Kinetica instance will have its own gadmin running and an additional
  //gadmin HA version can be run from another machine or one of the existing Kinetica machines with
  //a different application name(i.e gadmin-ha).  If the application name is changed the /gadmin/proxy
  //and /gadmin/external/proxy will need to change as well to /gadmin-ha/proxy and /gadmin-ha/external/proxy for example.
  //Enable HA tab by:
  //1. Adding the ha-sys-config.json to the home directory in this gadmin application
  //2. Copy over the ha-config.json file from one of the ha systems running gadmin into the ha-sys-config.json file.
  //      It will be in the home directory of the gadmin application.  If it is not there,
  //      start the ha process on the coresponding node and the config file will be created in the gadmin directory.
  //3. Add the following properties to the new ha-sys-config.json file and set the values:
  //      a). child.ha.process.url corresponds to the ha process url for the child.store
  //      b). brothers.ha.process.urls corresponds to the ha process urls for the brothers.stores, make sure to
  //          maintain the same order as the brothers.stores
  //      c). child.ha.gadmin.url corresponds to the gadmin application url for the child.store
  //      d). brothers.ha.gadmin.urls corresponds to the gadmin application urls for the brothers.stores, make sure to
  //          maintain the same order as the brothers.stores
  //      e). child.ha.gstats.url corresponds to the gstats application url for the child.store
  //      f). brothers.ha.gstats.urls corresponds to the gstats application urls for the brothers.stores, make sure to
  //          maintain the same order as the brothers.stores
  //4. Change gpudb_url in gaia.properties file to point load balancer
  //5. Copy over the rabbitmq.properties file from one of the ha systems running gadmin. The file is automatically
  //      generated by ha when it is started.  Copy from gadmin/WEB-INF/classes into this gadmin application
  //      /gadmin/WEB-INF/classes directory. If the file does not exist on the machine, start the ha service to generate
  //      file.


  var haConfigJsonFile = HA_ENABLED ? "ha-sys-config.json" : "noconfig.json";

  // Don't change DEFAULT_PROXY_*, this is so we can access it from inside if the PROXY_URL doesn't work
  var DEFAULT_PROXY_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/proxy/";
  var DEFAULT_PROXY_URL_EXTERNAL = window.location.origin + "/" + WEBAPP_CONTEXT + "/external/proxy/";
  var DEFAULT_PROXY_GRAFANA_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/stats";
  var DEFAULT_PROXY_HOSTMANAGER_URL = window.location.origin + "/" + WEBAPP_CONTEXT + "/hostmanager/proxy/";
  var DEFAULT_PROXY_HOSTMANAGER_URL_EXTERNAL = window.location.origin + "/" + WEBAPP_CONTEXT + "/external/hostmanager/proxy/";

  var GPUDB_API_TIMEOUT = 1000 * 3600; //sets Kinetica API timeoutMilliseconds
  var GPUDB_API_TIMEOUT_V2 = 0; // Force override of previous default with 0 as infinite timeout

  // Pass through HTTP authentication from browser to GPUdb proxy
  var PASSTHROUGH_AUTHENTICATION = false;

  //var GRAPHITE_URL = window.location.origin + "/gadmin/graphiteproxy/"; // URL to Graphite proxy servlet -- comment out to disable Graphite

  // GRAPHITE_METRICS defines what is in the tree of metrics on the Stats page.
  // Set to one of the following formats:
  //
  //      "Namespace"
  //
  //          Includes all of the sub-namespaces and metrics under the specified
  //          namespace in Graphite in the tree (use an empty string for the root
  //          namespace)
  //
  //      { namespace: "Namespace" }
  //
  //          Same as "Namespace"
  //
  //      { title: "Title", namespace: "Namespace" }
  //
  //          Creates a parent node with the specified title and includes all of
  //          the sub-namespaces and metrics under the specified namespace in
  //          Graphite under that parent node (use an empty string for the root
  //          namespace)
  //
  //      { metric: "Metric" }
  //
  //          Includes the specified metric in the tree (metric may be any metric
  //          or function that Graphite is able to render, specified in Graphite
  //          render API format)
  //
  //      { title: "Title", metric: "Metric" }
  //
  //          Includes the specified metric in the tree but uses the specified
  //          title (note that Graphite will not use the specified title; use the
  //          alias() function in the metric to change the title in Graphite)
  //
  //      { child: <child> }
  //
  //          Includes the specified child in the tree, where <child> is one of
  //          the formats described in this section
  //
  //      { title: "Title", child: <child> }
  //
  //          Creates a parent node with the specified title and includes the
  //          specified child under that parent node, where <child> is one of the
  //          formats described in this section
  //
  //      [ <child>, <child>, ... ]
  //
  //          Includes all of the specified children in the tree in the order
  //          specified, where each <child> is one of the formats described in
  //          this section

  var GRAPHITE_METRICS = "";

  // GRAPHITE_DEFAULTMETRICS defines what metrics are shown in the chart by
  // default. It is an array of strings, and each string is a metric or function
  // that Graphite is able to render, specified in Graphite render API format.

  var GRAPHITE_DEFAULTMETRICS = [];

  //All available Kinetica APIs. To add more, just append them.
  //The name will be added into the function list (drop down list) in the Query page.
  //The description will be shown in Query Parameters box and it's copied from http://gpudb.com/http-endpoints/

  var QUERY_FUNCTIONS = [
          { "name": "Bounding Box", "description": "The Bounding Box function allows you to query for all objects that fall within the rectangular parameters provided." },
          { "name": "Filter By Bounds", "description": "The filter by bounds function is the mechanism to calculate which objects from a set have an attribute that is within the given bounds." },
          { "name": "Filter By List", "description": "The 'filter by list' function is the mechanism to calculate which objects from a set have attributes whose values are in a given list." },
          { "name": "Filter By String", "description": "The 'filter by string' function is the mechanism to calculate which objects from a set match a string expression for the string attributes." },
          { "name": "Filter By Value", "description": "The 'filter by value' function is the mechanism to calculate which objects from a set has a particular value for a particular attribute." },
          { "name": "Histogram", "description": "The histogram function performs a histogram calculation given a set, an attribute, and an interval function." },
          { "name": "Max Min", "description": "The max-min function returns the maximum and minimum values of a particular attribute in a set." },
          { "name": "Select", "description": "The Select function creates a new set with the GUID result_set_id consisting of elements of set_id for which the given expression is true." }
  ];

  var DATA_VIEW_NULL_VALUES = '<null>';
  var DATA_VIEW_INVALID_DATE_VALUES = '<Invalid Timestamp(<REPLACE_WITH_DATE>)>';
  var ALLOW_NULLABLE_COLUMN_CREATION = true;

  var DEFAULT_SAMPLE_DATASET = {
          name: 'NYC Taxi',
          description: 'Trip records capture pick-up and drop-off dates/times, locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts.',
          thumbnail: 'img/taxi.png',
          loadingAnim: 'img/taxi_anim.gif',
          fileLocation: '',
          tableName: 'nyctaxi',
          schema: 'demo',
          dashboardName: 'NYC Taxi',
          dashboardData: 'nyctaxi.db',
          numRows: 500000,
          requiresTextSearch: false
  };

  var SYSTEM_STATUS_POLLING_INTERVAL = 6000;

  var LICENSE = {
          TYPE: {
                  UNLIMITED: 'unlimited',
                  KEY: 'key',
                  COMMUNITY: 'key-community',
                  TRIAL: 'key-trial',
                  DEVELOPER: 'developer-edition',
                  ENTERPRISE: 'key-enterprise',
                  CLOUD: 'cloud',
                  SERVER: 'server',
                  UNKNOWN: 'unknown'
          },
          STATUS: {
                  OK: 'ok',
                  INVALID: 'invalid',
                  EXPIRED: 'expired',
                  MISSING: 'missing'
          },
          LABEL: {
                  'unlimited': 'Unlimited',
                  'key': 'Key Required',
                  'key-community': 'Trial',
                  'key-trial': 'Trial',
                  'developer-edition': 'Developer',
                  'key-enterprise': 'Enterprise',
                  'cloud': 'Cloud',
                  'server': 'Server',
                  'ok': 'Valid',
                  'ok locally': 'Valid',
                  'invalid': 'Invalid',
                  'expired': 'Expired',
                  'missing': 'Missing',
                  'unknown': 'Key Required'
          }
  };

  var SYSTEM_STATUS = {
          RUNNING: 'running',
          STOPPED: 'stopped'
  };

  var DOCS_ENABLED = true;
  var DOCS_URL = window.location.origin + '/docs';

  var REVEAL_PORT = '8088';
  var REVEAL_DOCS = DOCS_URL + '/analytics/reveal/index.html';

  var KAGENT_PORT = '8081';

  var LOG_LEVELS = ['ERROR', 'WARN', 'INFO', 'DEBUG', 'TRACE', 'FATAL', 'OFF'];

  var GPUDB_STATUS_STORE_KEY = 'isGpudbActive';

  // Enable/disable display of support menu pages
  var SUPPORT_ENABLED = true;

  // Regex for detecting errors from KiSQL CLI tool
  var KISQL_SQL_ERROR_REGEX = /\[GPUdb\]executeSql: Error/g;

  // Params for fetching metrics from stats server
  var FROM_PARAM = '-30s';
  var UNTIL_PARAM = 'now';
  var MAXDATAPOINT_PARAM = '1';

  // Params for KiFS Browser
  var KIFS_COLLECTION = 'filesystem';
  var KIFS_FOLDER_PREFIX = '__kifs__';
  var KIFS_ROOT_LABEL = 'Home';
  var KIFS_MAX_UPLOAD_SIZE = 1000000000;

  // Info popup data
  var INFO_POPUP_TOPICS = {
          service: {
                  title: 'Service',
                  content: 'When <strong>Reshard</strong> is checked, rebalance will also cause \
                  the shards to be (as much as possible) equally distributed across all the ranks. \
                  During rebalancing, users may experience slower request response times.'
          },
          add_ranks: {
                  title: 'Add Ranks',
                  content: 'Add an entry for <strong>each</strong> rank to be added. \
                  When <strong>Reshard</strong> is checked, some of the shards from all the existing \
                  ranks will be moved to the new rank being added. \
                  During resharding, users may experience slower request response times.<br /><br />\
                  <strong>GPU Index</strong><br />\
                  Set GPU devices for each worker rank\'s TaskCalculators, see \'tcs_per_tom\'.\
                  If no gpus are specified, each rank\'s TaskCalculators will share the same\
                  GPU and each rank will round-robin the available gpus on the system.\
                  Ideally, each rank should use a single specified GPU to allow data caching.\
                  Add rankN.taskcalc_gpu as needed, where N ranges from 1 to the highest\
                  index in \'rankN.host\'.<br /><br />\
                  <strong>Base Numa</strong><br />\
                  Set each worker rank\'s preferred base numa node for CPU affinity and memory\
                  allocation. The \'rankN.base_numa_node\' is the node or nodes that non-data\
                  intensive threads will run in. These nodes do not have to be the same numa\
                  nodes that the GPU specified by the corresponding \'rankN.taskcalc_gpu\' is on\
                  for best performance, though they should be relatively near to their\
                  \'rankN.data_numa_node\'.<br /><br />\
                  <strong>Data Numa</strong><br />\
                  Set each worker rank\'s preferred data numa node for CPU affinity and memory\
                  allocation. The \'rankN.data_numa_node\' is the node or nodes that data intensive\
                  threads will run in and should be set to the same numa node that the GPU\
                  specified by the corresponding \'rankN.taskcalc_gpu\' is on for best performance.\
                  If the \'rankN.taskcalc_gpu\' is specified the \'rankN.data_numa_node\' will be\
                  automatically set to the node the GPU is attached to, otherwise\
                  there will be no CPU thread affinity or preferred node for memory allocation\
                  if not specified or left empty.<br /><br />\
                  <strong>HTTP Port</strong><br />\
                  Specify the worker HTTP server port.<br /><br />\
                  <strong>Text Index Address</strong><br />\
                  Use if \'use_external_text_server = true\'.\
                  Specify one for each worker rank N, where N ranges from 1 to highest index in \'rankN.host\'.\
                  Add the appropriate number of rankN.text_index_address for each worker rank as needed.\
                  The addresses can be a fully qualified TCP address:port for remote servers or\
                  an IPC address for local text index servers.<br /><br />\
                  If no addresses are specified the text index servers will use IPC and be started\
                  on the machine where the rank is running as shown in the IPC example below.\
                  You should either specify all addresses or none to get the defaults.'
          },
          remove_ranks: {
                  title: 'Remove Ranks',
                  content: 'When <strong>Reshard</strong> is checked, shards from selected ranks will \
                  be moved to the other ranks in the cluster. When unchecked, the rank will only be \
                  removed from the cluster if the rank does not contain any data shards, otherwise an \
                  error is returned. During resharding, users may experience slower request response \
                  times.<br /><br />When <strong>Force</strong> is checked, the rank is immediately \
                  shut down and removed from the cluster. This will result in loss of any data that \
                  is present in the node at the time of the request. The shards from the removed rank \
                  will be equally distributed to the remaining ranks.'
          }
  };

  // Username, password, and role name limits
  var USERNAME_LENGTH_LIMIT = 64;
  var PASSWORD_LENGTH_LIMIT = 1024;
  var ROLENAME_LENGTH_LIMIT = 64;

  var ENABLE_LOGIN_ANIMATION = true;

  // KiSQL related parameters
  var KISQL_RESPONSE_SIZE_LIMIT = 10485760; // bytes
  var KISQL_ENABLE_MUTATE_CONFIRMATION = false;
  var KISQL_ENABLE_CONFIRMATION_OPTOUT = true;
  var KISQL_MUTATE_KEYWORDS = ['alter', 'delete', 'drop', 'truncate', 'replace'];

  // Custom cluster page title/label
  // If title specified, will override default cluster/ring name
  // Color value format '#336699'
  var CLUSTER_PAGE_TITLE = '';
  var CLUSTER_PAGE_TITLE_COLOR = '';

  // Limit response string column value length calling get records from collection
  // Setting to 0 is equivalent to unlimited length
  var GET_RECORD_FROMCOLLECTION_STRING_LENGTH_LIMIT = 512;
  var CHARACTERS_TRUNCATED_KEY = 'characters truncated';

  // Custom raster tile service provider to replace OSM
  var CUSTOM_BASEMAP_URL = '';

  // Limit some features for free saas
  var FREE_SAAS = true;
gpudb_conf.tmpl: |
  # ==============================================================================
  # Kinetica configuration file.
  #
  # Created by the Kinetica DB Operator.
  # ==============================================================================

  [gaia]

  # ==============================================================================
  # Version

  # The current version of this configuration. This param is automatically updated
  # by Kinetica and should not be edited manually.
  file_version = 7.1.9.15.20230821195353.ga

  # ==============================================================================
  # Identification

  # Name of the ring containing the high availability clusters.
  ring_name = {{ .Cluster.Spec.GPUDBCluster.ParentHARing }}

  # Name of the cluster containing all the nodes of the database.
  cluster_name = {{ .Cluster.Spec.GPUDBCluster.Name }}

  # ==============================================================================
  # Hosts
  #
  # Specify each unique host in the cluster. Each host runs a local instance
  # of HostManager, which manages database services and and components that
  # are part of the Kinetica install.
  #
  # Host settings are defined as follows:
  #
  ## host<#>.<parameter>
  #
  # Any number of hosts may be configured, starting with 'host0', and must
  # be specified in consecutive order (e.g., 'host0', 'host1', 'host2').
  #
  # NOTE: For 'public_address' & 'host_manager_public_url', if either parameter is
  #       to be specified, that parameter must be specified for all hosts.

  # Host *parameters* include the following:
  #
  # * 'address'                : The unique address for this host. Single host
  #                              clusters may use 127.0.0.1. This field is required.
  # * 'public_address'         : An optional public address that clients should
  #                              use when performing multi-head operations.
  # * 'ha_address'             : An optional address to allow inter-cluster
  #                              communication with HA when 'address' is not routable
  #                              between clusters.
  # * 'host_manager_public_url': An optional public URL that can be used to access
  #                              HostManager endpoints. See 'host_manager_http_port'.
  # * 'ram_limit'              : An optional upper limit for RAM (in bytes) on this
  #                              host, capping the amount of memory that all Kinetica
  #                              processes are allowed to consume.
  #                              Use "-1" for no limit.
  # * 'gpus'                   : An optional comma-separated list of GPUs (as
  #                              reported by NVML) that may be reserved by Kinetica
  #                              services (e.g., ranks, graph server, ML, etc.).
  #                              Leave empty to make all GPUs available for use.
  #                              The same failover caveat with the RAM limit also
  #                              applies.
  # * 'accepts_failover'       : Whether or not this host should accept failed
  #                              processes that need to be migrated off degraded
  #                              hosts.  Default is to not accept failed processes.
  #
  # NOTE: For 'public_address' & 'host_manager_public_url', if either parameter is
  #       to be specified, that parameter must be specified for all hosts.

  #host0.address = 127.0.0.1
  #host0.public_address =
  #host0.ha_address =
  #host0.host_manager_public_url =
  host0.ram_limit = -1
  host0.gpus =
  host0.accepts_failover = false
  #host0.ha_address = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HAAddress }}
  host0.ha_address =

  # ==============================================================================
  # Ranks

  # Specify the host on which to run each rank worker process in the cluster.
  # Multiple ranks may run on the same host. If a rank is specified with an empty
  # host entry, it is effectively disabled (i.e., removed from the cluster) and
  # no persisted data for this rank will be loaded even if present on any host.
  # (e.g. 'rank2.host = host1' to run rank2 on host1)
  #
  #rank0.host = host0
  #rank1.host = host0
  #rank2.host = host0


  # ==============================================================================
  # Network

  # Head HTTP server IP address.
  # Set to the publicly accessible IP address of the first process, **rank0**.
  head_ip_address = ${gaia.host0.address}

  # Head HTTP server port to use for 'head_ip_address'.
  # head_port = 9191
  head_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HeadPort }}

  # Set to "true" to use HTTPS; if "true" then 'https_key_file' and
  # 'https_cert_file' must be provided
  # use_https = false
  use_https = {{ .Cluster.Spec.GPUDBCluster.Config.Network.UseHTTPS }}

  # Files containing the SSL private Key and the SSL certificate for.
  # If required, a self signed certificate (expires after 10 years) can be
  # generated via the command:
  #
  ## openssl req -newkey rsa:2048 -new -nodes -x509 \
  ##   -days 3650 -keyout key.pem -out cert.pem
  #
  https_key_file =
  https_cert_file =

  # Value to return via Access-Control-Allow-Origin HTTP header
  # (for Cross-Origin Resource Sharing).
  # Set to empty to not return the header and disallow CORS.
  http_allow_origin = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HTTPAllowOrigin }}

  # Keep HTTP connections alive between requests
  #http_keep_alive = false
  http_keep_alive = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HTTPKeepAlive }}

  # Start an HTTP server as a proxy to handle LDAP and/or Kerberos authentication.
  # Each host will run an HTTP server and access to each rank is available through
  # http://host:8082/gpudb-1, where port "8082" is defined by 'httpd_proxy_port'.
  #
  # NOTE: HTTP external endpoints are not affected by the 'use_https' parameter
  #       above. If you wish to enable HTTPS, you must edit the
  #       "/opt/gpudb/httpd/conf/httpd.conf" and setup HTTPS as per the Apache
  #       httpd documentation at https://httpd.apache.org/docs/2.2/
  # enable_httpd_proxy = true
  enable_httpd_proxy = {{ .Cluster.Spec.GPUDBCluster.Config.Network.EnableHTTPDProxy }}

  # TCP port that the httpd auth proxy server will listen on if
  # 'enable_httpd_proxy' is "true".
  # httpd_proxy_port = 8082
  httpd_proxy_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HTTPDProxyPort }}

  # Set to "true" if the httpd auth proxy server is configured to use HTTPS.
  # httpd_proxy_use_https = false
  httpd_proxy_use_https = {{ .Cluster.Spec.GPUDBCluster.Config.Network.HTTPDProxyUseHTTPS }}

  # Trigger ZMQ publisher server port ("-1" to disable), uses the
  # 'head_ip_address' interface.
  trigger_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.TriggerPort }}

  # Set monitor ZMQ publisher server port ("-1" to disable), uses the 'head_ip_address' interface.
  # set_monitor_port = 9002
  set_monitor_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.SetMonitorPort }}

  # Set monitor ZMQ publisher internal proxy server port ("-1" to disable), uses
  # the 'head_ip_address' interface.
  #
  # IMPORTANT:  Disabling this port effectively prevents worker nodes from
  # publishing set monitor notifications when multi-head ingest is enabled (see
  # 'enable_worker_http_servers').
  #set_monitor_proxy_port = 9003
  set_monitor_proxy_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.SetMonitorProxyPort }}

  # Set monitor queue size
  #set_monitor_queue_size = 1000
  set_monitor_queue_size = {{ .Cluster.Spec.GPUDBCluster.Config.Network.SetMonitorQueueSize }}

  # Enable Reveal runtime
  enable_reveal = {{ .Cluster.Spec.Reveal.IsEnabled }}

  # Internal communication ports
  # global_manager_port_one = 5552
  global_manager_port_one = {{ .Cluster.Spec.GPUDBCluster.Config.Network.GlobalManagerPortOne }}

  # Host manager synchronization port
  #global_manager_pub_port = 5553
  global_manager_pub_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.GlobalManagerPubPort }}

  # Local host manager port
  #global_manager_local_pub_port = 5554
  global_manager_local_pub_port = {{ .Cluster.Spec.GPUDBCluster.Config.Network.GlobalManagerLocalPubPort }}

  # HTTP port for web portal of the host manager
  #host_manager_http_port = 9300
  host_manager_http_port =  {{ .Cluster.Spec.GPUDBCluster.Config.Network.HostManagerHTTPPort }}

  # Enable worker HTTP servers; each process runs its own server for multi-head
  # ingest.
  #enable_worker_http_servers = true
  enable_worker_http_servers = {{ .Cluster.Spec.GPUDBCluster.Config.Network.EnableWorkerHTTPServers  }}

  # Optionally, specify the worker HTTP server ports.
  # The default is to use ('head_port' + *rank #*) for each worker process where
  # rank number is from "1" to number of ranks in 'rank<#>.host' below.

  #rank1.worker_http_server_port = 9192
  #rank2.worker_http_server_port = 9193


  # Optionally, specify a public URL for each worker HTTP server that clients
  # should use to connect for multi-head operations.
  #
  # NOTE: If specified for any ranks, a public URL must be specified for all
  # ranks. Cannot be used in conjunction with nplus1.

  #rank0.public_url =
  #rank1.public_url =
  #rank2.public_url =


  # Specify the TCP ports each rank will use to communicate with the others.
  # If the port for any 'rank<#>' is not specified the port will be assigned to
  # 'rank0.communicator_port' + *rank #*.

  rank0.communicator_port = 6555
  #rank1.communicator_port = 6556
  #rank2.communicator_port = 6557

  # Enables compression of inter-node network data transfers.
  compress_network_data = false


  # ==============================================================================
  # Security

  # Require authentication.
  require_authentication = {{ .Cluster.Spec.GPUDBCluster.Config.Security.RequireAuthentication }}
  #require_authentication = true

  # Enable authorization checks.
  enable_authorization = {{ .Cluster.Spec.GPUDBCluster.Config.Security.EnableAuthorization }}
  #enable_authorization = true

  # Minimum password length.
  min_password_length = {{ .Cluster.Spec.GPUDBCluster.Config.Security.MinPasswordLength }}

  # Enable external (LDAP, Kerberos, etc.) authentication. User IDs of
  # externally-authenticated users must be passed in via the "REMOTE_USER" HTTP
  # header from the authentication proxy. May be used in conjuntion with the
  # 'enable_httpd_proxy' setting above for an integrated external authentication
  # solution.
  #
  # IMPORTANT: DO NOT ENABLE unless external access to Kinetica ports
  # has been blocked via firewall AND the authentication proxy is
  # configured to block "REMOTE_USER" HTTP headers passed in from clients.
  enable_external_authentication = {{ .Cluster.Spec.GPUDBCluster.Config.Security.EnableExternalAuthentication }}
  #enable_external_authentication = true

  # Encrypted key that, if specified, must be passed in unencrypted via the
  # "KINETICA_HANDSHAKE_KEY" HTTP header from the authentication proxy if a
  # "REMOTE_USER" HTTP header is also passed in. A missing or incorrect handshake
  # key will result in rejection of the request.
  external_authentication_handshake_key =

  # Use a single namespace for internal and external user IDs and role names. If
  # false, external user IDs must be prefixed with "@" to differentiate them from
  # internal user IDs and role names (except in the "REMOTE_USER" HTTP header,
  # where the "@" is omitted).
  #unified_security_namespace = true
  unified_security_namespace =  {{ .Cluster.Spec.GPUDBCluster.Config.Security.UnifiedSecurityNamespace }}

  # Automatically create accounts for externally-authenticated users.
  # If 'enable_external_authentication' is "false", this setting has no effect.
  # Note that accounts are not automatically deleted if users are removed
  # from the external authentication provider and will be orphaned.
  auto_create_external_users = {{ .Cluster.Spec.GPUDBCluster.Config.Security.AutoCreateExternalUsers }}
  #auto_create_external_users = false

  # Automatically add roles passed in via the "KINETICA_ROLES" HTTP header to
  # externally-authenticated users. Specified roles that do not exist are
  # ignored. If 'enable_external_authentication' is "false", this setting has no
  # effect.
  #
  # IMPORTANT: DO NOT ENABLE unless the authentication proxy is
  # configured to block "KINETICA_ROLES" HTTP headers passed in from clients.
  auto_grant_external_roles = {{ .Cluster.Spec.GPUDBCluster.Config.Security.AutoGrantExternalRoles }}
  #auto_grant_external_roles = true

  # Comma-separated list of roles to revoke from externally-authenticated
  # users prior to granting roles passed in via the "KINETICA_ROLES" HTTP
  # header, or "*" to revoke all roles. Preceding a role name with an "!"
  # overrides the revocation (e.g. "*,!foo" revokes all roles except "foo").
  # Leave blank to disable. If either 'enable_external_authentication' or
  # 'auto_grant_external_roles' is "false", this setting has no effect.
  auto_revoke_external_roles =  {{ .Cluster.Spec.GPUDBCluster.Config.Security.AutoRevokeExternalRoles }}
  #auto_revoke_external_roles = true

  # ==============================================================================
  # External Security

  # URL of Ranger REST API.  E.g., https://localhost:6080/
  # Leave blank for no Ranger Server
  #security.external.ranger.url =
  {{ if .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.ExtURL }}security.external.ranger.url = {{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.ExtURL }}{{ else }}security.external.ranger.url = {{ end }}

  # Name of the service created on the Ranger Server to manage this Kinetica instance
  #security.external.ranger.service_name = kinetica
  security.external.ranger.service_name = {{ if .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.Name }}{{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.Name }}{{ else }}kinetica{{ end }}

  # Maximum minutes to hold on to data from Ranger
  #security.external.ranger.cache_minutes = 60
  security.external.ranger.cache_minutes = {{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.CacheMinutes }}

  # The network URI for the ranger_authorizer to start. The URI can be either TCP or IPC.
  # TCP address is used to indicate the remote ranger_authorizer which may run at other hosts.
  # The IPC address is for a local ranger_authorizer.
  #
  # Example addresses for remote or TCP servers:
  #
  ##  tcp://127.0.0.1:9293
  ##  tcp://HOST_IP:9293
  #
  # Example address for local IPC servers:
  #
  ##  ipc:///tmp/gpudb-ranger-0
  #security.external.ranger_authorizer.address = ipc://${gaia.temp_directory}/gpudb-ranger-0
  security.external.ranger_authorizer.address = {{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.AuthorizerAddress }}

  # Ranger Authorizer timeout in seconds
  #security.external.ranger_authorizer.timeout = 120
  security.external.ranger_authorizer.timeout = {{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.AuthorizerTimeout }}

  # Remote debugger port used for the ranger_authorizer. Setting the port to "0"
  # disables remote debugging.
  #
  # NOTE:  Recommended port to use is "5005"
  #security.external.ranger_authorizer.remote_debug_port = {{ .Cluster.Spec.GPUDBCluster.Config.Security.ExternalSecurity.Ranger.AuthorizerRemoteDebugPort }}


  # ==============================================================================
  # Auditing
  #
  # This section controls the request auditor, which will audit all requests
  # received by the server in full or in part based on the settings below.
  # The output location of the audited requests is controlled via settings in
  # the Auditing section of "gpudb_logger.conf".

  # Controls whether request auditing is enabled. If set to "true",
  # the following information is audited for every request: Job ID, URI, User,
  # and Client Address. The settings below control whether additional
  # information about each request is also audited. If set to "false", all
  # auditing is disabled.
  #enable_audit = false
  enable_audit = {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Enable }}

  # Controls whether HTTP headers are audited for each request.
  # If 'enable_audit' is "false" this setting has no effect.
  #audit_headers = false
  audit_headers =  {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Headers }}

  # Controls whether the body of each request is audited (in JSON
  # format). If 'enable_audit' is "false" this setting has no effect.
  #
  # NOTE: For requests that insert data records, this setting does not control the
  # auditing of the records being inserted, only the rest of the request body; see
  # 'audit_data' below to control this.
  #audit_body = false
  audit_body =  {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Body }}

  # Controls whether records being inserted are audited (in JSON
  # format) for requests that insert data records. If either 'enable_audit' or
  # 'audit_body' is "false", this setting has no effect.
  #
  # NOTE: Enabling this setting during bulk ingestion of data will rapidly produce
  # very large audit logs and may cause disk space exhaustion; use with caution.
  #audit_data = false
  audit_data = {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Data }}

  # Controls whether response information is audited for each request.
  # If 'enable_audit' is "false" this setting has no effect.
  #audit_response = false
  audit_response = {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Response }}

  # Controls whether the above audit settings can be altered at
  # runtime via the /alter/system/properties endpoint. In a secure
  # environment where auditing is required at all times, this should be set
  # to "true" to lock the settings to what is set in this file.
  #lock_audit = false
  lock_audit = {{ .Cluster.Spec.GPUDBCluster.Config.Audit.Lock }}


  # ==============================================================================
  # Licensing

  # The license key to authorize running.
  license_key = {{ .License }}


  # ==============================================================================
  # Processes and Threads

  # Set min number of web server threads to spawn. (default: "8")
  min_http_threads = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.MinHttpThreads }}

  # Set max number of web server threads to spawn. (default: "512")
  max_http_threads = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.MaxHttpThreads }}

  # Set the number of parallel jobs to create for multi-child set calculations.
  # Use "-1" to use the max number of threads (not recommended).
  sm_omp_threads = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.SmOmpThreads }}

  # Set the number of parallel calculation threads to use for data processing.
  # Use "-1" to use the max number of threads (not recommended).
  kernel_omp_threads = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.KernelOmpThreads }}

  # Set the maximum number of threads per tom for table initialization on gpudb startup
  init_tables_num_threads_per_tom = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.InitTablesNumThreadsPerTom }}

  # Set the maximum number of threads (both workers and masters) to be passed
  # to TBB on initialization.  Generally speaking,
  # 'max_tbb_threads_per_rank' - "1" TBB workers will be created.  Use "-1" for no
  # limit.
  max_tbb_threads_per_rank = {{ if .Cluster.Spec.GPUDBCluster.Config.Processes.MaxTbbThreadsPerRank }}{{ .Cluster.Spec.GPUDBCluster.Config.Processes.MaxTbbThreadsPerRank }}{{ else }}-1{{ end }}

  # Set the number of TOMs (data container shards) per rank
  toms_per_rank = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.TomsPerRank }}

  # Set the number of TaskProcessors per TOM, CPU data processors.
  tps_per_tom = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.TpsPerTom }}

  # Set the number of TaskCalculators per TOM, GPU data processors.
  tcs_per_tom = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.TcsPerTom }}

  # Maximum number of simultaneous threads allocated to a given request, on each rank.
  # Note that thread allocation may also be limted by resource group limits and/or system load.
  subtask_concurrency_limit = {{ .Cluster.Spec.GPUDBCluster.Config.Processes.SubtaskConcurrencyLimit }}

  # ==============================================================================
  # Hardware

  # Specify the GPU to use for all calculations on the HTTP server node,
  # **rank0**.
  #
  # NOTE: The **rank0** GPU may be shared with another rank.
  rank0.gpu = 0

  # Set the GPU device for each worker rank to use. If no GPUs are specified,
  # each rank will round-robin the available GPUs per host system.
  # Add 'rank<#>.taskcalc_gpu' as needed for the worker ranks, where *#* ranges
  # from "1" to the highest *rank #* among the 'rank<#>.host' parameters
  #
  # Example setting the GPUs to use for ranks 1 and 2:
  #
  ##   rank1.taskcalc_gpu = 0
  ##   rank2.taskcalc_gpu = 1

  #rank1.taskcalc_gpu =
  #rank2.taskcalc_gpu =


  # Set the head HTTP **rank0** numa node(s).
  # If left empty, there will be no thread affinity or preferred memory node.
  # The node list may be either a single node number or a range; e.g., "1-5,7,10".
  #
  # If there will be many simultaneous users, specify as many nodes as possible
  # that won't overlap the **rank1** to **rankN** worker numa nodes that the GPUs
  # are on.
  #
  # If there will be few simultaneous users and WMS speed is important, choose
  # the numa node the 'rank0.gpu' is on.

  rank0.numa_node =

  # Set each worker rank's preferred base numa node for CPU affinity and memory
  # allocation. The 'rank<#>.base_numa_node' is the node or nodes that non-data
  # intensive threads will run in. These nodes do not have to be the same numa
  # nodes that the GPU specified by the corresponding 'rank<#>.taskcalc_gpu' is on
  # for best performance, though they should be relatively near to their
  # 'rank<#>.data_numa_node'.
  #
  # There will be no CPU thread affinity or preferred node for memory allocation
  # if not specified or left empty.
  #
  # The node list may be a single node number or a range; e.g., "1-5,7,10".

  #rank1.base_numa_node =
  #rank2.base_numa_node =


  # Set each worker rank's preferred data numa node for CPU affinity and memory
  # allocation. The 'rank<#>.data_numa_node' is the node or nodes that data
  # intensive threads will run in and should be set to the same numa node that the
  # GPU specified by the corresponding 'rank<#>.taskcalc_gpu' is on for best
  # performance.
  #
  # If the 'rank<#>.taskcalc_gpu' is specified the 'rank<#>.data_numa_node' will
  # be automatically set to the node the GPU is attached to, otherwise
  # there will be no CPU thread affinity or preferred node for memory allocation
  # if not specified or left empty.
  #
  # The node list may be a single node number or a range; e.g., "1-5,7,10".

  #rank1.data_numa_node =
  #rank2.data_numa_node =


  # ==============================================================================
  # General

  # Tables and schemas with these names will not be deleted (comma separated).
  #protected_sets = MASTER,_MASTER,_DATASOURCE
  #protected_sets = {{ .Cluster.Spec.GPUDBCluster.Config.General.ProtectedSets }}

  # Time-to-live in minutes of non-protected tables before they are automatically
  # deleted from the database.
  default_ttl = {{ .Cluster.Spec.GPUDBCluster.Config.General.DefaultTTL }}

  # Disallow the /clear/table request to clear all tables.
  disable_clear_all = {{ .Cluster.Spec.GPUDBCluster.Config.General.DisableClearAll }}

  # Size in bytes of the pinned memory pool per-rank process to speed up copying
  # data to the GPU.  Set to "0" to disable.
  pinned_memory_pool_size = {{ .Cluster.Spec.GPUDBCluster.Config.General.PinnedMemoryPoolSize }}

  # Enable (if "true") multiple kernels to run concurrently on the same GPU
  concurrent_kernel_execution = {{ .Cluster.Spec.GPUDBCluster.Config.General.ConcurrentKernelExecution }}

  # Maximum number of kernels that can be running at the same time on a given
  # GPU. Set to "0" for no limit. Only takes effect if
  # 'concurrent_kernel_execution' is "true"
  max_concurrent_kernels = {{ .Cluster.Spec.GPUDBCluster.Config.General.MaxConcurrentKernels }}

  # If "true" then all filter execution will be host-only (i.e. CPU). This can be useful
  # for high-concurrency situations and when PCIe bandwidth is a limiting factor.
  force_host_filter_execution = {{ .Cluster.Spec.GPUDBCluster.Config.General.ForceHostFilterExecution }}

  # Maximum number of records that data retrieval requests such as
  # /get/records and /aggregate/groupby will return per request.
  max_get_records_size = {{ .Cluster.Spec.GPUDBCluster.Config.General.MaxGetRecordsSize }}

  # Timeout (in minutes) for filter-type requests
  request_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.General.RequestTimeout }}

  # Set an optional executable command that will be run once when Kinetica is
  # ready for client requests. This can be used to perform any initialization
  # logic that needs to be run before clients connect. It will be run as the
  # "gpudb" user, so you must ensure that any required permissions are set on the
  # file to allow it to be executed.  If the command cannot be executed or returns
  # a non-zero error code, then Kinetica will be stopped.  Output from the startup
  # script will be logged to "/opt/gpudb/core/logs/gpudb-on-start.log" (and its
  # dated relatives).  The "gpudb_env.sh" script is run directly before the
  # command, so the path will be set to include the supplied Python runtime.
  #
  # Example:
  #
  ## on_startup_script = /bin/ks arg1 arg2 ...
  #on_startup_script =
  {{ if .Cluster.Spec.GPUDBCluster.Config.General.OnStartupScript }}on_startup_script = {{ .Cluster.Spec.GPUDBCluster.Config.General.OnStartupScript }}{{ else }}on_startup_script = {{ end }}


  # Enable predicate-equi-join filter plan type
  enable_predicate_equi_join = {{ .Cluster.Spec.GPUDBCluster.Config.General.EnablePredicateEquiJoin }}

  # Enable overlapped-equi-join filters
  enable_overlapped_equi_join = {{ .Cluster.Spec.GPUDBCluster.Config.General.EnableOverlappedEquiJoin }}

  # Timeout (in seconds) to wait for each database subsystem to startup.
  # Subsystems include the Query Planner, Graph, Stats, & HTTP servers, as well
  # as external text-search ranks.
  timeout_startup_subsystem = {{ .Cluster.Spec.GPUDBCluster.Config.General.TimeoutStartupSubsystem }}

  # Timeout (in seconds) to wait for each database subsystem to exit gracefully
  # before it is force-killed.
  timeout_shutdown_subsystem = {{ .Cluster.Spec.GPUDBCluster.Config.General.TimeoutShutdownSubsystem }}

  # Timeout (in seconds) to wait for a rank to start during a cluster event (ex: failover)
  # event is considered failed.
  {{ if .Cluster.Spec.GPUDBCluster.Config.General.ClusterEventTimeoutStartupRank }}cluster_event_timeout_startup_rank = {{ .Cluster.Spec.GPUDBCluster.Config.General.ClusterEventTimeoutStartupRank.IntVal }}{{ else }}cluster_event_timeout_startup_rank = 300{{ end }}

  # Timeout (in seconds) to wait for a rank to exit gracefully before it is
  # force-killed. Machines with slow disk drives may require longer times and data
  # may be lost if a drive is not responsive.
  {{ if .Cluster.Spec.GPUDBCluster.Config.General.TimeoutShutdownRank }}timeout_shutdown_rank = {{ .Cluster.Spec.GPUDBCluster.Config.General.TimeoutShutdownRank.IntVal }}{{ else }}timeout_shutdown_rank = 300{{ end }}


  # ==============================================================================
  # Visualization
  #
  # Several of these options interact when determining the system resources
  # required for visualization.  Worker ranks use "F(N+1)" bytes of GPU memory
  # (VRAM) and "F" bytes of main memory (RAM) where:
  #
  ## F = max_heatmap_size * max_heatmap_size * 6 bytes
  ## N = opengl_antialiasing_level
  #
  # For example, when 'max_heatmap_size' is "3072", and 'opengl_antialasing_level'
  # is "0", 56.6 MB of VRAM are required.  When 'opengl_antialasing_level' is "4",
  # 283 MB are required.

  # Threshold number of points (per-TOM) at which point rendering switches to fast mode.
  point_render_threshold = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.PointRenderThreshold }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.PointRenderThreshold }}{{ else }}100000{{ end }}

  # Threshold for the number of points (per-TOM) after which symbology rendering
  # falls back to regular rendering
  symbology_render_threshold = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbologyRenderThreshold }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbologyRenderThreshold }}{{ else }}10000{{ end }}

  # Maximum heatmap size (in pixels) that can be generated. This reserves
  # 'max_heatmap_size' ^ 2 * 8 bytes of GPU memory at **rank0**
  max_heatmap_size = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxHeatmapSize }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxHeatmapSize }}{{ else }}3072{{ end }}

  # The image width/height (in pixels) of svg symbols cached in the OpenGL symbol cache.
  symbol_resolution = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbolResolution }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbolResolution }}{{ else }}100{{ end }}

  # The width/height (in pixels) of an OpenGL texture which caches symbol images for OpenGL rendering.
  symbol_texture_size = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbolTextureSize }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.SymbolTextureSize }}{{ else }}4000{{ end }}

  # If "true", enable hardware-accelerated OpenGL renderer; if "false", use the
  # software-based Cairo renderer.
  enable_opengl_renderer = {{ .Cluster.Spec.GPUDBCluster.Config.Visualization.EnableOpenGLRenderer }}

  # The number of samples to use for antialiasing. Higher numbers will improve
  # image quality but require more GPU memory to store the samples on worker
  # ranks.  This affects only the OpenGL renderer.
  #
  # Value may be "0", "4", "8" or "16".  When "0" antialiasing is disabled.
  # The default value is "0".
  opengl_antialiasing_level = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.OpenGLAntialiasingLevel }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.OpenGLAntialiasingLevel }}{{ else }}0{{ end }}

  # Single-precision coordinates are used for usual rendering processes, but depending on
  # the precision of geometry data and use case, double precision processing may be required
  # at a high zoom level. Double precision rendering processes are used from the zoom level
  # specified by this parameter, which is corresponding to a zoom level of TMS or Google map service.
  rendering_precision_threshold = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.RenderingPrecisionThreshold }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.RenderingPrecisionThreshold }}{{ else }}30{{ end }}

  # Enable level-of-details rendering for fast interaction with large WKT polygon
  # data.  Only available for the OpenGL renderer (when 'enable_opengl_renderer'
  # is "true").
  enable_lod_rendering = {{ .Cluster.Spec.GPUDBCluster.Config.Visualization.EnableLODRendering }}

  # If "true", enable Vector Tile Service (VTS) to support client-side visualization
  # of geospatial data. Enabling this option increases memory usage on ingestion.
  enable_vectortile_service = {{ .Cluster.Spec.GPUDBCluster.Config.Visualization.EnableVectorTileService }}

  # Input geometries are pre-processed upon ingestion for faster vector tile generation.
  # This parameter determines the zoom level from which the vector tile pre-processing starts.
  # A vector tile request for a lower zoom level than this parameter takes additional time
  # because the vector tile needs to be generated on the fly.
  min_vectortile_zoomlevel = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MinVectorTileZoomLevel }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MinVectorTileZoomLevel }}{{ else }}1{{ end }}

  # Input geometries are pre-processed upon ingestion for faster vector tile generation.
  # This parameter determines the zoom level at which the vector tile pre-processing stops.
  # A vector tile request for a higher zoom level than this parameter takes additional time
  # because the vector tile needs to be generated on the fly.
  max_vectortile_zoomlevel = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxVectorTileZoomLevel }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxVectorTileZoomLevel }}{{ else }}8{{ end }}

  # The name of map tiler used for Vector Tile Service.
  # "google" and "tms" map tilers are supported currently.
  # This parameter should be matched with the map tiler of clients' vector tile renderer.
  vectortile_map_tiler = {{ .Cluster.Spec.GPUDBCluster.Config.Visualization.VectorTileMapTiler }}

  # Hidden parameter: Avoid trying to generate vector tiles on huge amounts of geometry.
  max_vertices_per_chunk = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxVerticesPerChunk }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxVerticesPerChunk }}{{ else }}2000000{{ end }}

  # Hidden parameter: Control the maximum number of features returned per tile (per chunk);
  # features beyond this count will be skipped.
  max_features_per_tile = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxFeaturesPerTile }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxFeaturesPerTile }}{{ else }}200000{{ end }}

  # Longitude and latitude ranges of geospatial data for which level-of-details
  # representations are being generated. The parameter order is:
  #
  ## <min_longitude> <min_latitude> <max_longitude> <max_latitude>
  #
  # The default values span over the world, but the level-of-details rendering
  # becomes more efficient when the precise extent of geospatial data is
  # specified.
  #lod_data_extent = -180 -90 180 90
  lod_data_extent ={{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.LODDataExtent }}{{ range .Cluster.Spec.GPUDBCluster.Config.Visualization.LODDataExtent }} {{.}}{{end}} {{ else }} -180 -90 180 90{{ end }}

  # The number of subregions in horizontal and vertical geospatial data extent.
  # The default values of "12 6" divide the world into subregions of
  # 30 degree (lon.) x 30 degree (lat.)
  #lod_subregion_num = 12 6
  lod_subregion_num ={{ range .Cluster.Spec.GPUDBCluster.Config.Visualization.LODSubRegionNum }} {{.}}{{end}}

  # A base image resolution (width and height in pixels) at which a subregion
  # would be rendered in a global view spanning over the whole dataset. Based on
  # this resolution level-of-details representations are generated for the
  # polygons located in the subregion.
  #lod_subregion_resolution = 512 512
  lod_subregion_resolution ={{ range .Cluster.Spec.GPUDBCluster.Config.Visualization.LODSubRegionResolution }} {{.}}{{end}}

  # The maximum number of levels in the level-of-details rendering. As the number
  # increases, level-of-details rendering becomes effective at higher zoom levels,
  # but it may increase memory usage for storing level-of-details representations.
  #max_lod_level = 8
  max_lod_level = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxLODLevel }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.MaxLODLevel }}{{ else }}8{{ end }}

  # The extent to which shape data are pre-processed for level-of-details
  # rendering during data insert/load or processed on-the-fly in rendering time.
  # This is a trade off between speed and memory. The higher the value, the faster
  # level-of-details rendering is, but the more memory is used for storing
  # processed shape data.
  #
  # The maximum level is "10" (most shape data are pre-processed) and the minimum
  # level is "0".
  #lod_preprocessing_level = 5
  lod_preprocessing_level = {{ if .Cluster.Spec.GPUDBCluster.Config.Visualization.LODPreProcessingLevel }}{{ .Cluster.Spec.GPUDBCluster.Config.Visualization.LODPreProcessingLevel }}{{ else }}5{{ end }}


  # ==============================================================================
  # Video

  # System default TTL for videos.
  # Time-to-live (TTL) is the number of minutes before a video will expire and be
  # removed, or -1 to disable.
  #video_default_ttl = -1
  video_default_ttl = {{ if .Cluster.Spec.GPUDBCluster.Config.Video.DefaultTTL }}{{ .Cluster.Spec.GPUDBCluster.Config.Video.DefaultTTL.IntVal }}{{ else }}-1{{ end }}

  # The maximum number of videos to allow on the system. Set to 0 to disable video
  # rendering.  Set to -1 to allow an unlimited number of videos.
  #video_max_count = -1
  video_max_count = {{ if .Cluster.Spec.GPUDBCluster.Config.Video.MaxCount }}{{ .Cluster.Spec.GPUDBCluster.Config.Video.MaxCount.IntVal }}{{ else }}-1{{ end }}


  # Directory where video files should be temporarily stored while rendering.
  # Only accessed by rank 0.
  #video_temp_directory = ${gaia.temp_directory}/gpudb-temp-videos
  video_temp_directory = {{ if .Cluster.Spec.GPUDBCluster.Config.Video.TmpDir }}{{ .Cluster.Spec.GPUDBCluster.Config.Video.TmpDir }}{{ else }}${gaia.temp_directory}/gpudb-temp-videos{{ end }}


  # ==============================================================================
  # GAdmin
  #
  # GAdmin is a web-based UI for administering the Kinetica DB and system.

  # Enable Tomcat web-server for the GAdmin UI
  enable_tomcat = true


  # ==============================================================================
  # Workbench
  #
  # Workbench is a web-based UI for managing database objects and writing rich SQL
  # workbooks in Kinetica.

  # Start the Workbench app on the head host when host manager is started.
  #enable_workbench = false
  enable_workbench = {{ .Cluster.Spec.GPUDBCluster.Config.Workbench.Enable }}

  # HTTP server port for Workbench if enabled.
  workbench_port = {{ .Cluster.Spec.GPUDBCluster.Config.Workbench.Port.ContainerPort }}


  # ==============================================================================
  # Text Search

  # Enable text search capability within the database.
  enable_text_search = {{ .Cluster.Spec.GPUDBCluster.Config.TextSearch.EnableTextSearch }}

  # Use the production capable external text server instead of a lightweight
  # internal server which should only be used for light testing.
  #
  # NOTE: The internal text server is deprecated and may be removed in future versions.
  use_external_text_server = {{ .Cluster.Spec.GPUDBCluster.Config.TextSearch.UseExternalTextServer }}

  # Number of text indices to start for each rank
  text_indices_per_tom = {{ .Cluster.Spec.GPUDBCluster.Config.TextSearch.TextIndicesPerTom }}

  # Searcher refresh intervals - specifies the maximum delay (in
  # seconds) between writing to the text search index and being able to
  # search for the value just written.  A value of "0" insures that writes
  # to the index are immediately available to be searched.  A more
  # nominal value of "100" should improve ingest speed at the cost
  # of some delay in being able to text search newly added values.
  text_searcher_refresh_interval = {{ .Cluster.Spec.GPUDBCluster.Config.TextSearch.TextSearcherRefreshInterval }}

  # External text server addresses to use if 'use_external_text_server' is "true".
  #
  # Specify one for each worker 'rank<#>', where *#* ranges from "1" to highest
  # index in 'rank<#>.host'. Add the appropriate number of
  # 'rank<#>.text_index_address' for each worker rank as needed. The addresses can
  # be a fully qualified TCP *address:port* for remote servers or an IPC address
  # for local text index servers.
  #
  # If no addresses are specified, the text index servers will use IPC and be
  # started on the machine where the rank is running as shown in the IPC example
  # below. You should either specify all addresses or none to get the defaults.
  #
  # Example for remote or TCP servers:
  #
  ##  rank1.text_index_address  = tcp://127.0.0.1:4000
  ##  rank2.text_index_address  = tcp://127.0.0.1:4001
  ##  ... up to rank<N>.text_index_address = ...
  #
  # Example for local IPC servers:
  #
  ##  rank1.text_index_address  = ipc:///tmp/gpudb-text-index-1
  ##  rank2.text_index_address  = ipc:///tmp/gpudb-text-index-2
  ##  ... up to rank<N>.text_index_address = ...
  #
  # Where "/tmp/gpudb-text-index-1" is the name of the socket file to create.

  #rank1.text_index_address  = ipc://${gaia.temp_directory}/gpudb-text-index-1
  #rank2.text_index_address  = ipc://${gaia.temp_directory}/gpudb-text-index-2


  # ==============================================================================
  # Persistence

  # Specify a base directory to store persistence data files.
  persist_directory = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.PersistDirectory }}

  # Base directory to store hashed strings.
  #sms_directory = ${gaia.persist_directory}
  sms_directory = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.SMSDirectory }}

  # Base directory to store the text search index.
  #text_index_directory = ${gaia.persist_directory}
  text_index_directory = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.TextIndexDirectory }}

  # Directory for Kinetica to use to store temporary files.
  # Must be a fully qualified path, have at least 100Mb of free space, and execute
  # permission.
  temp_directory = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.TempDirectory }}

  # Whether or not to use synchronous persistence file writing.  If "false", files
  # will be written asynchronously.
  # Removed in 7.2
  # persist_sync = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.PersistSync}}

  # Flush the row store in persist after every update.
  # Removed in 7.2
  # indexdb_flush_immediate = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBFlushImmediate }}

  # Flush metadata files in persist after every update.
  # Removed in 7.2
  # metadata_flush_immediate = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.MetadataFlushImmediate }}

  # Fsync the row store in persist whenever files are asynchronously flushed to disk.
  # See 'persist_sync_time' for setting the interval.
  # Removed in 7.2
  # fsync_on_interval = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.FSyncOnInterval }}

  # Fsync the row store in persist after every update to ensure it is fully written.
  # Removed in 7.2
  # fsync_indexdb_immediate = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.FSyncIndexDBImmediate }}

  # Fsync the metadata files in persist after every update to ensure it is fully written.
  # fsync_metadata_immediate = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.FSyncMetadataImmediate }}
  # Removed in 7.2
  # fsync_metadata_rank0_immediate = true

  # Fsync directories in persist after every update to ensure filesystem toc is up to date.
  # Removed in 7.2
  # fsync_inodes_immediate = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.FSyncInodesImmediate }}

  # Maximum number of open metadata stores per rank.
  # metadata_store_pool_size = 128

  # Sync mode to use when persisting metadata stores to disk:
  #
  # * "off"       : Turn off synchronous writes by default
  # * "normal"    : Use normal synchronous writes by default
  # * "full"      : Use full synchronous writes by default
  # metadata_store_sync_mode = normal

  # Compress data in memory immediately (or in the background) when compression is
  # applied to an existing table's column(s) via /alter/table
  synchronous_compression = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.SynchronousCompression }}

  # The maximum time in seconds a secondary column store persist data file can be out of
  # sync with memory. Set to a very high number to disable forced syncing.
  persist_sync_time = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.PersistSyncTime }}

  # Startup data-loading scheme:
  #
  # * "always"    : load as much of the stored data as possible into memory before
  #                 accepting requests
  # * "lazy"      : load the necessary data to start, and load as much of the
  #                 remainder of the stored data as possible into memory lazily
  # * "on_demand" : only load data as requests use it
  load_vectors_on_start = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.LoadVectorsOnStart }}
  build_pk_index_on_start = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.BuildPKIndexOnStart }}
  build_materialized_views_on_start = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.BuildMaterializedViewsOnStart }}

  # Maximum number of active automatic view updators
  #max_auto_view_updators = 3

  # Table of contents size for IndexedDb object file store
  # Removed in 7.2
  # indexdb_toc_size = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBTOCSize }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBTOCSize }}{{ end }}

  # Maximum number of open files for IndexedDb object file store
  # Removed in 7.2
  # indexdb_max_open_files = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBMaxOpenFiles }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBMaxOpenFiles }}{{ end }}

  # Disable detection of sparse file support and use the full file length
  # which may be an over-estimate of the actual usage in the persist tier.
  # Removed in 7.2
  # indexdb_tier_by_file_length = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.IndexDBTierByFileLength }}

  wal.max_segment_size = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALMaxSegmentSize }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALMaxSegmentSize }}{{else}}500000000{{end}}
  wal.segment_count = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALSegmentCount }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALSegmentCount }}{{else}}-1{{end}}
  wal.sync_policy = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALSyncPolicy }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALSyncPolicy }}{{else}}"flush"{{end}}
  wal.flush_frequency = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALFlushFrequency }} {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALFlushFrequency }}{{else}}60{{end}}
  wal.checksum = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALChecksum }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALChecksum }}{{else}}true{{end}}
  wal.truncate_corrupt_tables_on_start = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.WALTruncateCorruptTablesOnStart }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.WALTruncateCorruptTablesOnStart }}{{else}}false{{end}}
  chunk_column_max_memory = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.ChunkColumnMaxMemory }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ChunkColumnMaxMemory }}{{else}}512000000{{end}}
  chunk_max_memory = {{ if .Cluster.Spec.GPUDBCluster.Config.Persistence.ChunkMaxMemory }}{{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ChunkMaxMemory }}{{else}}8192000000{{end}}

  # Maximum number of open files (per-TOM) for the SMS (string) store
  sms_max_open_files = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.SMSMaxOpenFiles }}

  # Number of records per chunk ("0" disables chunking)
  chunk_size = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ChunkSize }}

  # Determines whether to execute kernels on host (CPU) or device (GPU). Possible values are:
  #
  # * "default"   : engine decides
  # * "host"      : execute only host
  # * "device"    : execute only device
  # * *<rows>*    : execute on the host if chunked column contains the given
  #                 number of *rows* or fewer; otherwise, execute on device.
  execution_mode = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ExecutionMode }}

  # Whether or not to enable chunk caching
  shadow_cube_enabled = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ShadowCubeEnabled }}

  # The maximum number of bytes in the shadow aggregate cache
  shadow_agg_size = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ShadowAggSize }}

  # The maximum number of bytes in the shadow filter cache
  shadow_filter_size = {{ .Cluster.Spec.GPUDBCluster.Config.Persistence.ShadowFilterSize }}

  # ==============================================================================
  # Statistics

  # Run a statistics server to collect information about Kinetica and the machines
  # it runs on.
  enable_stats_server = {{ .Cluster.Spec.GPUDBCluster.Config.Statistics.Enable }}

  # Statistics server IP address (run on head node) default port is "2003"
  stats_server_ip_address = {{ .Cluster.Spec.GPUDBCluster.Config.Statistics.IPAddress }}
  stats_server_port = {{ .Cluster.Spec.GPUDBCluster.Config.Statistics.Port }}

  # Statistics server namepace - should be a machine identifier
  stats_server_namespace = {{ .Cluster.Spec.GPUDBCluster.Config.Statistics.Namespace }}

  # Use the internal event and metric server on the head host server
  # if true, otherwise use the Kagent services.
  # Note that Kagent installs will automatically set this value to false.
  event_server_internal = {{ .Cluster.Spec.GPUDBCluster.Config.Events.Internal }}

  # Event collector server address and port.
  event_server_address = {{ .Cluster.Spec.GPUDBCluster.Config.Events.IPAddress }}
  event_server_port = {{ .Cluster.Spec.GPUDBCluster.Config.Events.Port }}

  # Enable running promtail to parse and send logs to the event_server_address.
  enable_promtail = false

  # Alertmanager server address and port.
  alertmanager_address = {{ .Cluster.Spec.GPUDBCluster.Config.AlertManager.IPAddress }}
  alertmanager_port = {{ .Cluster.Spec.GPUDBCluster.Config.AlertManager.Port }}

  # Fluentbit TCP server address and port with a json format parser.
  fluentbit_address = ${gaia.host0.address}
  fluentbit_port = 5170

  # ==============================================================================
  # Procs

  # Enable procs (UDFs)
  enable_procs = {{ .Cluster.Spec.GPUDBCluster.Config.Procs.Enable }}

  # Directory where proc files are stored at runtime. Must be a fully qualified
  # path with execute permission. If not specified, 'temp_directory' will be used.
  proc_directory = {{ .ProcDirectory }}

  # Directory where data transferred to and from procs is written. Must be a
  # fully qualified path with sufficient free space for required volume of data.
  # If not specified, 'temp_directory' will be used.
  proc_data_directory = {{ .ProcDataDirectory }}


  # ==============================================================================
  # Graph Servers
  #
  # One or more graph servers can be created across available hosts.

  # ------------------------------------------------------------------------------
  # Global Parameters

  # Enable/Disable all graph operations
  enable_graph_server = {{ .Cluster.Spec.GPUDBCluster.Config.Graph.Enable }}

  # Specify where the graph server should be run, defaults to head node
  graph_server_ip_address = {{ .Cluster.Spec.GPUDBCluster.Config.Graph.IPAddress }}

  # Port used for requests from the database server to the graph server
  graph_server_push_port = {{ .Cluster.Spec.GPUDBCluster.Config.Graph.PushPort }}

  # Port used for responses from the graph server to the database server
  graph_server_pull_port = {{ .Cluster.Spec.GPUDBCluster.Config.Graph.PullPort }}

  # List of GPU devices to be used by graph server
  # The server would ideally be run on a different node with dedicated GPU(s)
  graph_gpu_list =

  # Maximum memory (in MB) that can be used by the graph server, set to "0" to disable
  # memory restriction
  graph_max_memory = {{ .Cluster.Spec.GPUDBCluster.Config.Graph.MaxMemory }}


  # ==============================================================================
  # KiFS

  # Enable access to the KiFS file system from within procs (UDFs). This will
  # create a "filesystem" schema and an internal "kifs" user in Kinetica
  # and mount the file system at the mount point specified below. By default,
  # the file system is accessible to the "gpudb_proc" user; to make it accessible
  # to other users, you must ensure that 'user_allow_other' is enabled in
  # "/etc/fuse.conf" and add the other users to the "gpudb_proc" group.
  #
  # For instance:
  #
  ## sudo usermod -a -G gpudb_proc <user>
  #enable_kifs = {{ .Cluster.Spec.GPUDBCluster.Config.Kifs.Enable }}
  enable_kifs = false

  # Parent directory of the mount point for the KiFS file system. Must be a
  # fully qualified path. The actual mount point will be a subdirectory *mount*
  # below this directory. Note that this folder must have read, write and
  # execute permissions for the "gpudb" user and the "gpudb_proc" group, and it
  # cannot be a path on an NFS.
  kifs_mount_point = {{ .Cluster.Spec.GPUDBCluster.Config.Kifs.MountPoint }}

  # ==============================================================================
  # Etcd

  # List of accessible etcd server URLs.
  etcd_urls =

  # Encrypted login credential for Etcd at given URLs.
  etcd_auth_user =

  # ==============================================================================
  # HA
  #
  # Enable/Disable HA from here. All other parameters will be in etcd at the
  # address specified in the relevant section above.

  # Enable HA.
  enable_ha = false

  # ==============================================================================
  # Machine Learning (ML)

  # Enable the ML server.
  enable_ml = {{ .Cluster.Spec.GPUDBCluster.Config.ML.Enable }}

  # default ML API service port number
  ml_api_port = 9187

  # ==============================================================================
  # Alerts

  # Enable the alerting system.
  enable_alerts = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.EnableAlerts }}

  # Executable to run when an alert condition occurs. This executable will
  # only be run on **rank0** and does not need to be present on other nodes.
  alert_exe =
  #alert_exe = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertExe }}

  # Trigger an alert whenever the status of a host or rank changes.
  alert_host_status = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertHostStatus }}

  # Optionally, filter host alerts for a comma-delimited list of statuses.
  # If a filter is empty, every host status change will trigger an alert.
  alert_host_status_filter = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertHostStatusFilter }}

  # Trigger an alert whenever the status of a rank changes.
  alert_rank_status = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertRankStatus }}

  # Optionally, filter rank alerts for a comma-delimited list of statuses.
  # If a filter is empty, every rank status change will trigger an alert.
  alert_rank_status_filter = fatal_init_error, not_responding, terminated
  #alert_rank_status_filter = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertRankCudaError}}

  # Trigger an alert if a CUDA error occurs on a rank.
  alert_rank_cuda_error = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertRankCudaError }}

  # Trigger alerts when the fallback allocator is employed; e.g., host memory is
  # allocated because GPU allocation fails.
  #
  # NOTE:  To prevent a flooding of alerts, if a fallback allocator is triggered
  # in bursts, not every use will generate an alert.
  alert_rank_fallback_allocator = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertRankFallbackAllocator }}

  # Trigger generic error message alerts, in cases of various significant runtime
  # errors.
  alert_error_messages = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertErrorMessages }}

  # Trigger an alert if available memory on any given node falls to or below a
  # certain threshold, either absolute (number of bytes) or percentage of total
  # memory. For multiple thresholds, use a comma-delimited list of values.
  alert_memory_percentage = 20, 10, 5, 1
  alert_memory_absolute =
  #alert_memory_percentage = {{range $i,$a := .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertMemoryPercentage }} {{if gt $i 0 }}, {{$a}}{{else}}{{$a}}{{end}}{{end}}
  #alert_memory_absolute = {{range $i,$a := .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertMemoryAbsolute }} {{if gt $i 0 }}, {{$a}}{{else}}{{$a}}{{end}}{{end}}


  # Trigger an alert if available disk space on any given node falls to or below a
  # certain threshold, either absolute (number of bytes) or percentage of total
  # disk space. For multiple thresholds, use a comma-delimited list of values.
  alert_disk_percentage = 20, 10, 5, 1
  alert_disk_absolute =
  #alert_disk_percentage = {{range $i,$a := .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertDiskPercentage}}{{if gt $i 0 }}, {{$a}}{{else}}{{$a}}{{end}}{{end}}
  #alert_disk_absolute = {{range $i,$a := .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertDiskAbsolute}}{{if gt $i 0 }}, {{$a}}{{else}}{{$a}}{{end}}{{end}}


  # The maximum number of triggered alerts guaranteed to be stored at any given
  # time. When this number of alerts is exceeded, older alerts may be discarded to
  # stay within the limit.
  alert_max_stored_alerts = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.AlertMaxStoredAlerts }}

  # Directory where the trace event and summary files are stored.  Must be a
  # fully qualified path with sufficient free space for required volume of data.
  trace_directory = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.TraceDirectory }}

  # The maximum number of trace events to be collected
  trace_event_buffer_size = {{ .Cluster.Spec.GPUDBCluster.Config.Alerts.TraceEventBufferSize }}

  # ==============================================================================
  # Failover (NPlus1)

  # Whether or not the system should fail over failed processes on the head/worker
  # nodes to other nodes
  #
  # * "true"  : Initiate failover to another node if a process fails
  # * "false" : Do not initiate failover to another node (restart on the same node)
  np1.enable_head_failover = false
  np1.enable_worker_failover = false

  # Controls the failure threshold for processes before failover is attempted.
  # A process must exceed its restart attempts limit within the restart interval.
  # Number of successive times a process will be restarted
  np1.rank_restart_attempts = 1
  np1.critical_restart_attempts = 1
  np1.non_critical_restart_attempts = 3

  # Time interval in seconds since last restart after which restart count will be reset
  np1.restart_interval = 60

  # An api script is required for persistent storage which supports dynamic mounting, such as cloud-based storage.
  # This script is responsible for issuing the underlying api calls directly based on requests from the database.
  # If this field is blank it implies all hosts will have access to persist for all ranks at all times.
  np1.storage_api_script =

  # Enable concurrent mounting operations. By default, multiple attach or detach
  # disk volume operations are permitted to run concurrently. Set to 'false' to limit to
  # running a single volume attach/detach operation on a host at a time.
  np1.enable_concurrent_mount_ops = true

  # Post-migration startup data-loading scheme:
  #
  # * "always"    : load as much of the stored data as possible into memory before
  #                 accepting requests
  # * "lazy"      : load the necessary data to start, and load as much of the
  #                 remainder of the stored data as possible into memory lazily
  # * "on_demand" : only load data as requests use it
  np1.load_vectors_on_migration = always
  np1.build_pk_index_on_migration = always
  np1.build_materialized_views_on_migration = always

  # Heartbeats are used to detect host-level failures.
  # The frequency in seconds that heartbeat requests are broadcasted to hosts
  heartbeat_interval = 20

  # The allowable window in seconds for heartbeat responses (must be less than heartbeat_interval)
  heartbeat_timeout = 10

  # The number of allowable consecutive missed heartbeats before the host is considered
  # degraded and any failover events begin
  heartbeat_missed_limit = 3

  # ==============================================================================
  # SQL Engine

  # Enable Query Planner
  sql.enable_planner = true

  # The network URI for the query planner to start. The URI can be either TCP or IPC.
  # TCP address is used to indicate the remote query planner which may run at other hosts.
  # The IPC address is for a local query planner.
  #
  # Example for remote or TCP servers:
  #
  ##  sql.planner.address  = tcp://127.0.0.1:9293
  ##  sql.planner.address  = tcp://HOST_IP:9293
  #
  # Example for local IPC servers:
  #
  ##  sql.planner.address  = ipc:///tmp/gpudb-query-engine-0
  sql.planner.address = ipc://${gaia.temp_directory}/gpudb-query-engine-0

  # Remote debugger port used for the query planner. Setting the port to "0"
  # disables remote debugging.
  #
  # NOTE:  Recommended port to use is "5005"
  sql.planner.remote_debug_port = 0

  # The maximum memory for the query planner to use in Megabytes.
  sql.planner.max_memory = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PlannerMaxMemory }}

  # The maximum stack size for the query planner threads to use in Megabytes.
  sql.planner.max_stack = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PlannerMaxStack }}

  # Query planner timeout in seconds
  sql.planner.timeout = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PlannerTimeout }}

  # Max Query planner threads
  sql.planner.workers = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PlannerWorkers }}

  # Enable query results caching
  sql.results.caching = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.ResultsCaching }}

  # TTL of the query cache results table
  sql.results.cache_ttl = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.ResultsCacheTTL }}

  # Perform joins between only 2 tables at a time;
  # default is all tables involved in the operation at once
  sql.force_binary_joins = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.ForceBinaryJoins }}

  # Perform unions/intersections/exceptions between only 2 tables at a time;
  # default is all tables involved in the operation at once
  sql.force_binary_set_ops = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.ForceBinarySetOps }}

  # The maximum number of entries in the SQL plan cache.  The default is "4000"
  # entries, but the configurable range is "1" - "1000000".  Plan caching will be
  # disabled if the value is set outside of that range.
  sql.plan_cache_size = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PlanCacheSize }}

  # Maximum number of entries in the SQL result cache.  The default is "4000"
  #sql.result_cache_size = 4000

  # Enable rule-based query rewrites
  sql.rule_based_optimization = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.RuleBasedOptimization }}

  # Enable the cost-based optimizer
  sql.cost_based_optimization = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.CostBasedOptimization }}

  # Enable distributed joins
  sql.distributed_joins = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.DistributedJoins }}

  # Enable distributed operations
  sql.distributed_operations = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.DistributedOperations }}

  # Enable parallel query evaluation
  sql.parallel_execution = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.ParallelExecution }}

  # Max parallel steps
  sql.max_parallel_steps = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.MaxParallelSteps }}

  # TTL of the paging results table
  sql.paging_table_ttl = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.PagingTableTTL }}

  # Max allowed view nesting levels. Valid range("1"-"64")
  sql.max_view_nesting_levels = {{ .Cluster.Spec.GPUDBCluster.Config.SQL.MaxViewNestingLevels }}

  # ==============================================================================
  # AI

  # AI API provider type. The default is "sqlgpt"
  #ai.api.provider = sqlgpt
  ai.api.provider = {{ .Cluster.Spec.GPUDBCluster.Config.AI.APIProvider.String }}

  # AI API url. The default is "https://sqlgpt.io/api/sql/suggest"
  #ai.api.url =
  ai.api.url = {{ if .Cluster.Spec.GPUDBCluster.Config.AI.APIURL }}{{.Cluster.Spec.GPUDBCluster.Config.AI.APIURL}}{{ else }}{{ end }}

  # AI API key.
  #ai.api.key =
  ai.api.key = {{ if .Cluster.Spec.GPUDBCluster.Config.AI.APIKey }}{{.Cluster.Spec.GPUDBCluster.Config.AI.APIKey}}{{ else }}{{ end }}

  # ==============================================================================
  # External Files

  # Defines the directory from which external files can be loaded
  external_files_directory = {{ .Cluster.Spec.GPUDBCluster.Config.ExternalFiles.Directory }}

  # Maximum number of simultaneous threads allocated to a given external file read
  # request, on each rank. Note that thread allocation may also be limited by
  # resource group limits, the 'subtask_concurrency_limit' setting, or system
  # load.
  external_file_reader_num_tasks = {{ .Cluster.Spec.GPUDBCluster.Config.ExternalFiles.ReaderNumTasks }}

  # Max file size (in MB) to allow saving to a single file.
  # May be overridden by target limitations.
  #egress_single_file_max_size = 100
  egress_single_file_max_size =  {{ .Cluster.Spec.GPUDBCluster.Config.ExternalFiles.EgressSingleFileMaxSize }}

  # Parquet files compression type
  #egress_parquet_compression = {{ .Cluster.Spec.GPUDBCluster.Config.ExternalFiles.EgressParquetCompression }}

  # Maximum number of records to be ingested in a single batch
  #kafka.batch_size = 1000
  kafka.batch_size =  {{ .Cluster.Spec.GPUDBCluster.Config.Kafka.BatchSize }}

  # Maximum wait time (seconds) to buffer records received from kafka before ingestion
  #kafka.wait_time = 30
  kafka.wait_time = {{ .Cluster.Spec.GPUDBCluster.Config.Kafka.WaitTime }}

  # Maximum time (milliseconds) for each poll to get records from kafka
  #kafka.poll_timeout = 0
  kafka.poll_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.Kafka.PollTimeout }}

  # System metadata catalog settings
  #system_metadata.stats_retention_days = 21
  system_metadata.stats_retention_days = {{ if .Cluster.Spec.GPUDBCluster.Config.Statistics.RetentionDays }}{{ .Cluster.Spec.GPUDBCluster.Config.Statistics.RetentionDays }}{{else}}21{{end}}
  #system_metadata.stats_aggr_rowcount = 10000
  system_metadata.stats_aggr_rowcount = {{ if .Cluster.Spec.GPUDBCluster.Config.Statistics.AggrRowCount }}{{ .Cluster.Spec.GPUDBCluster.Config.Statistics.AggrRowCount }}{{else}}10000{{end}}
  #system_metadata.stats_aggr_time = 1
  system_metadata.stats_aggr_time = {{ if .Cluster.Spec.GPUDBCluster.Config.Statistics.AggrTime }}{{ .Cluster.Spec.GPUDBCluster.Config.Statistics.AggrTime }}{{else}}1{{end}}

  # ==============================================================================
  # Tiered Storage
  #
  # Defines system resources using a set of containers (tiers) in which data can
  # be stored, either on a temporary (memory) or permanent (disk) basis. Tiers
  # are defined in terms of their maximum capacity and water mark thresholds.
  # The general format for defining tiers is:
  #
  ## tier.<tier_type>.<config_level>.<parameter>
  #
  # where *tier_type* is one of the five basic types of tiers:
  #
  # * 'vram'    : GPU memory
  # * 'ram'     : Main memory
  # * 'disk'    : Disk cache
  # * 'persist' : Permanent storage
  # * 'cold'    : Extended long-term storage
  #
  # Each tier can be configured on a global or per-rank basis, indicated by the
  # *config_level*:
  #
  # * 'default' : global, applies to all ranks, must be accessible by all hosts
  # * 'rank<#>' : local, applies only to the specified rank, overriding any global
  #               default
  #
  # If a field is not specified at the 'rank<#>' level, the specified 'default'
  # value applies. If neither is specified, the global system defaults will take
  # effect, which vary by tier.
  #
  # The parameters are also tier-specific and will be listed in their respective
  # sections, though every tier, except Cold Storage, will have the following:
  #
  # * 'limit'          : [ "-1", "1" .. "N" ] (bytes)
  # * 'high_watermark' : [ "1" .. "100" ] (percent)
  # * 'low_watermark'  : [ "1" .. "100" ] (percent)
  #
  # NOTE:  To disable watermark-based eviction, set the 'low_watermark' and
  # 'high_watermark' values to "100". Watermark-based eviction is also ignored
  # if the tier limit is set to -1 (no limit).

  # ------------------------------------------------------------------------------
  # Global Tier Parameters

  # Timeout in seconds for subsequent requests to wait on a locked resource
  #tier.global.concurrent_wait_timeout = 120
  tier.global.concurrent_wait_timeout =  {{ if and .Cluster.Spec.GPUDBCluster.Config.TieredStorage .Cluster.Spec.GPUDBCluster.Config.TieredStorage.GlobalTier }}{{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.GlobalTier.ConcurrentWaitTimeout  }}{{ else }}120{{ end }}

  # ------------------------------------------------------------------------------
  # VRAM Tier
  #
  # The VRAM Tier is composed of the memory available in one or multiple GPUs per
  # host machine.
  #
  # A default memory limit and eviction thresholds can be set for CUDA-enabled
  # devices across all ranks, while one or more ranks may be configured to
  # override those defaults.
  #
  # The general format for VRAM settings:
  #
  ## tier.vram.[default|rank<#>].all_gpus.<parameter>

  # Valid *parameter* names include:
  #
  # * 'limit'          : The maximum VRAM (bytes) per rank that can be allocated
  #                      on GPU(s) across all resource groups.  Default is "-1",
  #                      signifying to reserve 95% of the available GPU memory at
  #                      startup.
  # * 'high_watermark' : VRAM percentage used eviction threshold.  Once memory
  #                      usage exceeds this value, evictions from this tier will be
  #                      scheduled in the background and continue until the
  #                      'low_watermark' percentage usage is reached.  Default is
  #                      "90", signifying a 90% memory usage threshold.
  # * 'low_watermark'  : VRAM percentage used recovery threshold.  Once memory
  #                      usage exceeds the 'high_watermark', evictions will
  #                      continue until memory usage falls below this recovery
  #                      threshold. Default is "80", signifying an 80% memory usage
  #                      threshold.
  #
  tier.vram.default.all_gpus.limit = -1
  tier.vram.default.all_gpus.high_watermark = 90
  tier.vram.default.all_gpus.low_watermark = 80


  # ------------------------------------------------------------------------------
  # RAM Tier
  #
  # The RAM Tier represents the RAM available for data storage per rank.
  #
  # The RAM Tier is NOT used for small, non-data objects or variables that are
  # allocated and deallocated for program flow control or used to store metadata or
  # other similar information; these continue to use either the stack or the regular
  # runtime memory allocator. This tier should be sized on each machine such that
  # there is sufficient RAM left over to handle this overhead, as well as the needs
  # of other processes running on the same machine.
  #
  # A default memory limit and eviction thresholds can be set across all ranks,
  # while one or more ranks may be configured to override those defaults.
  #
  # The general format for RAM settings:
  #
  ## tier.ram.[default|rank<#>].<parameter>

  # Valid *parameter* names include:
  #
  # * 'limit'          : The maximum RAM (bytes) per rank that can be allocated
  #                      across all resource groups.  Default is "-1", signifying
  #                      to automatically set the maximum capacity as a portion of
  #                      total system memory or the host limit.
  # * 'high_watermark' : RAM percentage used eviction threshold.  Once memory
  #                      usage exceeds this value, evictions from this tier will be
  #                      scheduled in the background and continue until the
  #                      'low_watermark' percentage usage is reached.  Default is
  #                      "90", signifying a 90% memory usage threshold.
  # * 'low_watermark'  : RAM percentage used recovery threshold.  Once memory
  #                      usage exceeds the 'high_watermark', evictions will
  #                      continue until memory usage falls below this recovery
  #                      threshold. Default is "80", signifying an 80% memory usage
  #                      threshold.
  #
  tier.ram.default.limit = {{ .RAMTierSize }}
  {{ if and .Cluster.Spec.GPUDBCluster.Config.TieredStorage .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier.Default.HighWatermark }}
  tier.ram.default.high_watermark = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier.Default.HighWatermark }}
  {{ else }}tier.ram.default.high_watermark = {{ end }}
  {{ if and .Cluster.Spec.GPUDBCluster.Config.TieredStorage .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier.Default.LowWatermark }}
  tier.ram.default.low_watermark = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.RAMTier.Default.LowWatermark }}
  {{ else }}tier.ram.default.low_watermark = {{ end }}

  # The maximum RAM (bytes) for processing data at rank 0. Overrides the overall
  # default RAM tier limit.
  #tier.ram.rank0.limit = -1
  tier.ram.rank0.limit = {{ .Rank0RAMTierSize }}


  # ------------------------------------------------------------------------------
  # Disk Tier
  #
  # Disk Tiers are used as temporary swap space for data that doesn't fit in RAM
  # or VRAM. The disk should be as fast or faster than the Persist Tier storage
  # since this tier is used as an intermediary cache between the RAM and Persist
  # Tiers. Multiple Disk Tiers can be defined on different disks with different
  # capacities and performance parameters. No Disk Tiers are required, but they
  # can improve performance when the RAM Tier is at capacity.
  #
  # A default storage limit and eviction thresholds can be set across all ranks
  # for a given Disk Tier, while one or more ranks within a Disk Tier may be
  # configured to override those defaults.
  #
  # The general format for Disk settings:
  #
  ## tier.disk<#>.[default|rank<#>].<parameter>
  #
  # Multiple Disk Tiers may be defined such as 'disk', 'disk0', 'disk1', ... etc.
  # to support different tiering strategies that use any one of the Disk Tiers.  A
  # tier strategy can have, at most, one Disk Tier.  Create multiple tier
  # strategies to use more than one Disk Tier, one per strategy.  See
  # 'tier_strategy' parameter for usage.

  # Valid *parameter* names include:
  #
  # * 'path'           : A base directory to use as a swap space for this tier.
  # * 'limit'          : The maximum disk usage (bytes) per rank for this tier
  #                      across all resource groups.  Default is "-1", signifying
  #                      no limit and ignore watermark settings.
  # * 'high_watermark' : Disk percentage used eviction threshold.  Once disk
  #                      usage exceeds this value, evictions from this tier will be
  #                      scheduled in the background and continue until the
  #                      'low_watermark' percentage usage is reached.  Default is
  #                      "90", signifying a 90% disk usage threshold.
  # * 'low_watermark'  : Disk percentage used recovery threshold.  Once disk usage
  #                      exceeds the 'high_watermark', evictions will continue
  #                      until disk usage falls below this recovery threshold.
  #                      Default is "80", signifying a 80% disk usage threshold.
  # * 'store_persistent_objects' :
  #                      If true, allow the disk cache to store copies of data even if
  #                      they are already stored in a persistent tier (persist/cold).
  #
  # Example 'default' disk cache configuration using 'disk0':
  #
  ## tier.disk0.default.path = /opt/gpudb/diskcache_0
  ## tier.disk0.default.limit = -1
  ## tier.disk0.default.high_watermark = 90
  ## tier.disk0.default.low_watermark = 80
  ## tier.disk0.default.store_persistent_objects = false

  tier.disk0.default.path = {{ .TierDisk0DefaultPath }}
  tier.disk0.default.limit = {{ .DiskCacheSize }}
  #tier.disk0.default.high_watermark = 90
  #tier.disk0.default.low_watermark = 80
  #tier.disk0.default.store_persistent_objects = false


  # ------------------------------------------------------------------------------
  # Persist Tier
  #
  # The Persist Tier is a single pseudo-tier that contains data in persistent form
  # that survives between restarts. Although it also is a disk-based tier, its
  # behavior is different from Disk Tiers:  data for persistent objects is always
  # present in the Persist Tier (or Cold Storage Tier, if configured), but may not
  # be up-to-date at any given time.
  #
  # A default storage limit and eviction thresholds can be set across all ranks,
  # while one or more ranks may be configured to override those defaults.  The
  # Graph Solver engine may be given its own storage settings; however, it is not
  # tiered, and therefore cannot have limit/watermark settings applied.
  #
  # The general format for Persist settings:
  #
  ## tier.persist.[default|rank<#>|text<#>|graph<#>].<parameter>
  #
  # WARNING:  In general, limits on the Persist Tier should only be set if one or
  # more Cold Storage Tiers are configured.  Without a supporting Cold Storage
  # Tier to evict objects in the Persist Tier to, operations requiring space in
  # the Persist Tier will fail when the limit is reached.

  # Valid *parameter* names include:
  #
  # * 'path'           : Base directory to store column and object vectors.
  # * 'storage'        : The storage volume corresponding to the persist tier,
  #                      for managed storage volumes. Must be the 'vol<#>' for a
  #                      configured storage volume. Do not specify a 'default' as each
  #                      rank and graph server should have their own.
  #                      storage volumes (unlisted, as there is no default value).
  # * 'limit'          : The maximum disk usage (bytes) per rank for this tier
  #                      across all resource groups.  Default is "-1", signifying
  #                      no limit and ignore watermark settings.
  # * 'high_watermark' : Disk percentage used eviction threshold.  Once disk
  #                      usage exceeds this value, evictions from this tier to cold
  #                      storage (if configured) will be scheduled in the
  #                      background and continue until the 'low_watermark'
  #                      percentage usage is reached.  Default is "90", signifying
  #                      a 90% disk usage threshold.
  # * 'low_watermark'  : Disk percentage used recovery threshold.  Once disk usage
  #                      exceeds the 'high_watermark', evictions will continue
  #                      until disk usage falls below this recovery threshold.
  #                      Default is "80", signifying a 80% disk usage threshold.
  #
  # NOTE: 'path' and 'storage' are the only applicable parameters for 'text' and 'graph'
  #
  # Example showing a rank0 configuration:
  #
  ## tier.persist.rank0.path = /opt/data_rank0
  ## tier.persist.rank0.storage = vol0
  ## tier.persist.rank0.limit = -1
  ## tier.persist.rank0.high_watermark = 90
  ## tier.persist.rank0.low_watermark = 80

  tier.persist.default.path = ${gaia.persist_directory}
  tier.persist.default.limit = {{ .PersistSize }}
  tier.persist.default.high_watermark = {{ if and .Cluster.Spec.GPUDBCluster.Config.TieredStorage .Cluster.Spec.GPUDBCluster.Config.TieredStorage.PersistTier }}{{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.PersistTier.Default.HighWatermark }}{{ end }}
  tier.persist.default.low_watermark = {{ if and .Cluster.Spec.GPUDBCluster.Config.TieredStorage .Cluster.Spec.GPUDBCluster.Config.TieredStorage.PersistTier }}{{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.PersistTier.Default.LowWatermark }}{{ end }}


  # ------------------------------------------------------------------------------
  # Cold Storage Tier
  #
  # Cold Storage Tiers can be used to extend the storage capacity of the Persist Tier.
  # Assign a tier strategy with cold storage to objects that will be infrequently
  # accessed since they will be moved as needed from the Persist Tier.
  # The Cold Storage Tier is typically a much larger capacity physical disk or
  # a cloud-based storage system which may not be as performant as the Persist
  # Tier storage.
  #
  # A default storage limit and eviction thresholds can be set across all ranks
  # for a given Cold Storage Tier, while one or more ranks within a Cold Storage
  # Tier may be configured to override those defaults.
  #
  # NOTE: If an object needs to be pulled out of cold storage during a query,
  # it may need to use the local persist directory as a temporary swap space.
  # This may trigger an eviction of other persisted items to cold storage due to
  # low disk space condition defined by the watermark settings for the Persist Tier.
  #
  # The general format for Cold Storage settings:
  #
  ## tier.cold<#>.[default|rank<#>].<parameter>
  #
  # Multiple Cold Storage Tiers may be defined such as 'cold', 'cold0', 'cold1',
  # ... etc. to support different tiering strategies that use any one of the Cold
  # Storage Tiers. A tier strategy can have, at most, one Cold Storage Tier.
  # Create multiple tier strategies to use more than one Cold Storage Tier, one
  # per strategy.  See 'tier_strategy' parameter for usage.

  # Valid *parameter* names include:
  #
  # * 'type'                   : The storage provider type. Currently supports "disk"
  #                              (local/network storage), "hdfs" (Hadoop distributed filesystem),
  #                              "azure" (Azure blob storage), "s3" (Amazon S3 bucket) and "gcs"
  #                              (Google Cloud Storage bucket).
  # * 'base_path'              : A base path based on the provider type for this tier.
  # * 'wait_timeout'           : Timeout in seconds for reading from or writing to this
  #                              storage provider. This value should ideally be less than the value
  #                              for 'tier.global.concurrent_wait_timeout' to allow concurrent queries
  #                              sufficient time to acquire this resource during normal tiering
  #                              operations with some slack to accomodate the request.
  # * 'connection_timeout'     : Timeout in seconds for connecting to this storage provider.
  # * 'use_managed_credentials': If true, use cloud provider user settings from the environment.
  #                              If false, and no credentials are supplied, use anonymous access.
  #                              This option applies only to "azure", "gcs", and "s3" providers.
  # * 'use_https'              : This optional field can be used to override the default scheme for
  #                              the storage endpoint where applicable.
  #                              If true, use the 'https' scheme (default), otherwise use 'http'.
  #
  # HDFS-specific *parameter* names:
  #
  # * 'hdfs_uri'               : The host IP address & port for the hadoop distributed file system.
  #                              For example:  hdfs://localhost:8020
  # * 'hdfs_principal'         : The effective principal name to use when connecting to the hadoop
  #                              cluster.
  # * 'hdfs_use_kerberos'      : Set to "true" to enable Kerberos authentication to an HDFS storage
  #                              server. The credentials of the principal are in the file specified
  #                              by the 'hdfs_kerberos_keytab' parameter. Note that Kerberos's *kinit*
  #                              command will be run when the database is started.
  # * 'hdfs_kerberos_keytab'   : The Kerberos keytab file used to authenticate the "gpudb" Kerberos
  #                              principal.
  #
  # Amazon S3-specific *parameter* names:
  #
  # * 's3_bucket_name'
  # * 's3_region'                  (optional)
  # * 's3_endpoint'                (optional)
  # * 's3_aws_access_key_id'       (optional)
  # * 's3_aws_secret_access_key'   (optional)
  # * 's3_aws_role_arn'            (optional)
  # * 's3_encryption_type'     :   This is optional and valid values are sse-s3 (Encryption key is managed by Amazon S3)
  #                                and sse-kms (Encryption key is managed by AWS Key Management Service (kms)).
  # * 's3_kms_key_id'          :   This is optional and must be specified when encryption type is sse-kms.
  # * 's3_encryption_customer_algorithm' :   This is optional and must be specified when encryption type is sse-c.
  # * 's3_encryption_customer_key' :   This is optional and must be specified when encryption type is sse-c.
  # * 's3_use_virtual_addressing'  :   If true (default), S3 endpoints will be constructed using the 'virtual' style which
  #                                    includes the bucket name as part of the hostname. Set to false to use the 'path'
  #                                    style which treats the bucket name as if it is a path in the URI.
  #
  # NOTE: If 's3_aws_access_key_id' and/or 's3_aws_secret_access_key' values
  # are not specified, they may instead be provided by the AWS CLI or via the
  # respective 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' environment
  # variables.
  #
  # Microsoft Azure-specific *parameter* names:
  #
  # * 'azure_container_name'
  # * 'azure_storage_account_name'
  # * 'azure_endpoint'                    (optional) Specifies access to an Azure Private Link service.
  # * 'azure_storage_account_key'         (optional) An Azure 'Access key' linked to a Storage account.
  # * 'azure_sas_token'                   (optional) A Shared Access Signature token.
  #
  # Google Cloud Storage-specific *parameter* names:
  #
  # * 'gcs_bucket_name'
  # * 'gcs_project_id'                    (optional)
  # * 'gcs_service_account_id'            (optional)
  # * 'gcs_service_account_private_key'   (optional)
  # * 'gcs_service_account_keys'          (optional)
  #
  # NOTE: If the 'gcs_service_account_id', 'gcs_service_account_private_key' and/or
  # 'gcs_service_account_keys' values are not specified, the Google Clould Client
  # Libraries will attempt to find and use service account credentials from
  # the GOOGLE_APPLICATION_CREDENTIALS environment variable.

  #tier.cold0.default.type = disk
  #tier.cold0.default.base_path = /mnt/gpudb/cold_storage
  #tier.cold0.default.wait_timeout = 90
  #tier.cold0.default.connection_timeout = 1


  # ------------------------------------------------------------------------------
  # Tier Strategy

  # Default strategy to apply to tables or columns when one was not provided
  # during table creation. This strategy is also applied to a resource group
  # that does not specify one at time of creation.
  #
  # The strategy is formed by chaining together the tier types and their
  # respective eviction priorities. Any given tier may appear no more than once
  # in the chain and the priority must be in range "1" - "10", where "1" is the
  # lowest priority (first to be evicted) and "9" is the highest priority (last
  # to be evicted).  A priority of "10" indicates that an object is unevictable.
  #
  # Each tier's priority is in relation to the priority of other objects in the
  # same tier; e.g., "RAM 9, DISK2 1" indicates that an object will be the
  # highest evictable priority among objects in the RAM Tier (last evicted), but
  # that it will be the lowest priority among objects in the Disk Tier named
  # 'disk2' (first evicted).  Note that since an object can only have one Disk
  # Tier instance in its strategy, the corresponding priority will only apply in
  # relation to other objects in Disk Tier instance 'disk2'.
  #
  # See the Tiered Storage section for more information about tier type names.
  #
  # Format:
  #
  ## <tier1> <priority>, <tier2> <priority>, <tier3> <priority>, ...
  #
  # Examples using a Disk Tier named 'disk2' and a Cold Storage Tier 'cold0':
  #
  ## vram 3, ram 5, disk2 3, persist 10
  ## vram 3, ram 5, disk2 3, persist 6, cold0 10
  #
  #tier_strategy.default = VRAM 1, RAM 5, DISK0 5, PERSIST 5

  # Predicate evaluation interval (in minutes) -  indicates the interval at which
  # the tier strategy predicates are evaluated
  tier_strategy.predicate_evaluation_interval = 60

  # ------------------------------------------------------------------------------
  # Default Resource Group
  #
  # Resource groups are used to enforce simultaneous memory, disk and thread usage
  # limits for all users within a given group. Users not assigned to a specific
  # resource group will be placed within this default group. Tier-based limits are
  # applied on top of existing rank tier limits.

  # The scheduling priority for this group's operations, "1" - "100", where "1" is
  # the lowest priority and "100" is the highest
  resource_group.default.schedule_priority = 50

  # The maximum eviction priority for tiered objects, "1" - "10". This supercedes
  # any priorities that are set by any user provided tiering strategies.
  resource_group.default.max_tier_priority = 10

  # Maximum number of concurrent data operations; minimum is "4"; "-1" for no limit
  resource_group.default.max_cpu_concurrency = -1

  # The maximum memory (bytes) this group can use at any given time in the VRAM
  # tier; "-1" for no limit
  resource_group.default.vram_limit = -1

  # The maximum memory (bytes) this group can use at any given time in the RAM
  # tier; "-1" for no limit
  resource_group.default.ram_limit = -1

  # The maximum memory (bytes) this group can cumulatively use in the RAM
  # tier; "-1" for no limit
  #resource_group.default.data_limit = -1

  # ==============================================================================
  # Storage Volumes
  #
  # When persisted rank data is stored on attached external storage volumes, the
  # following config entries are used to automatically attach and mount the volume
  # upon migration of a rank to another host. Attaching and mounting is performed
  # by the script specified by np1.storage_api_script.
  #
  # The general format for storage volumes:
  #
  ## storage.volumes.vol<#>.<parameter>
  #
  # Volumes are numbered starting at zero and match the number of ranks plus
  # additional volumes to match graph servers.
  #
  # Valid *parameter* names include:
  #
  # * 'fs_uuid'               : The UUID identifier for the volume on the cloud
  #                             provider.
  # * 'id'                    : The cloud identifier of the volume.
  # * 'mount_point'           : The local path to mount the cloud volume.
  #                             The folder must exist and be owned by gpudb.
  #
  # Example:
  #
  ## storage.volumes.vol0.fs_uuid = my_vol_uuid
  ## storage.volumes.vol0.id = /subscriptions/my_az_subscription_uuid/resourceGroups/my_az_rg/providers/Microsoft.Compute/
  ## storage.volumes.vol0.mount_point = /opt/data_rank0


  fluentbit_address = ${gaia.host0.address}
  fluentbit_port = 5170
gpudb_conf_cold_storage_az_blob.tmpl: |
  tier.cold0.default.type = azure

  tier.cold0.default.base_path = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.BasePath }}
  tier.cold0.default.connection_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ConnectionTimeout }}
  tier.cold0.default.use_https = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.UseHttps }}
  tier.cold0.default.use_managed_credentials = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.UseManagedCredentials }}
  tier.cold0.default.wait_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.WaitTimeout }}

  tier.cold0.default.azure_container_name = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ContainerName}}
  tier.cold0.default.azure_storage_account_name = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.StorageAccountName}}
  tier.cold0.default.azure_storage_account_key = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.StorageAccountKey}}
  tier.cold0.default.azure_sas_token = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.SASToken}}

  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ClientID }}tier.cold0.default.azure_client_id = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ClientID }}{{ end }}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ClientSecret }}tier.cold0.default.azure_client_secret = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.ClientSecret }}{{ end }}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.TenantID }}tier.cold0.default.azure_tenant_id = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageAzure.TenantID }}{{ end }}
gpudb_conf_cold_storage_disk.tmpl: |-
  tier.cold0.default.type = disk
  tier.cold0.default.base_path = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageDisk.BasePath}}
  tier.cold0.default.wait_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageDisk.WaitTimeout}}
  tier.cold0.default.connection_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageDisk.ConnectionTimeout}}
gpudb_conf_cold_storage_hdfs.tmpl: |
  tier.cold0.default.type = hdfs
  tier.cold0.default.base_path = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.BasePath}}
  tier.cold0.default.wait_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.WaitTimeout}}
  tier.cold0.default.connection_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.ConnectionTimeout}}
  tier.cold0.default.hdfs_uri = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.URI}}
  tier.cold0.default.hdfs_principal = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.URI}}
  tier.cold0.default.hdfs_use_kerberos = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.UseKerberos}}
  tier.cold0.default.hdfs_kerberos_keytab = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageHDFS.KerberosKeytab}}
gpudb_conf_cold_storage_s3.tmpl: |2

  tier.cold0.default.type = s3

  tier.cold0.default.base_path = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.BasePath }}
  tier.cold0.default.connection_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.ConnectionTimeout }}
  tier.cold0.default.use_https = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseHttps }}
  tier.cold0.default.use_managed_credentials = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseManagedCredentials }}
  tier.cold0.default.wait_timeout = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.WaitTimeout }}

  tier.cold0.default.s3_aws_access_key_id = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.AWSAccessKeyId }}
  tier.cold0.default.s3_aws_secret_access_key = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.AWSSecretAccessKey }}
  tier.cold0.default.s3_bucket_name = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.BucketName }}
  tier.cold0.default.s3_endpoint = {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.Endpoint }}{{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.Endpoint }}{{ end }}
  tier.cold0.default.s3_region = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.Region }}
  tier.cold0.default.s3_use_virtual_addressing = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseVirtualAddressing }}

  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.AWSRoleARN }}tier.cold0.default.s3_aws_role_arn = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.AWSRoleARN }}{{ end }}
  {{/*  's3_encryption_type'     :   This is optional and valid values are sse-s3 (Encryption key is managed by Amazon S3) and sse-kms (Encryption key is managed by AWS Key Management Service (kms)) */}}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionType }}tier.cold0.default.s3_encryption_type = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionType.String }}{{ end }}
  {{/* 's3_kms_key_id'          :   This is optional and must be specified when encryption type is sse-kms */}}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.KMSKeyID }}tier.cold0.default.s3_kms_key_id = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.KMSKeyID }}{{ end }}

  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionCustomerAlgorithm }}tier.cold0.default.s3_encryption_customer_algorithm = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionCustomerAlgorithm }}{{ end }}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionCustomerKey }}tier.cold0.default.s3_encryption_customer_key = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.EncryptionCustomerKey }}{{ end }}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseManagedCredentials }}tier.cold0.default.use_managed_credentials = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseManagedCredentials }}{{ end }}
  {{/* If true (default), S3 endpoints will be constructed using the 'virtual' style which
       includes the bucket name as part of the hostname. Set to false to use the 'path'
       style which treats the bucket name as if it is a path in the URI */}}
  {{ if .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseVirtualAddressing }}tier.cold0.default.use_virtual_addressing = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.ColdStorageTier.ColdStorageS3.UseVirtualAddressing }}{{ end }}

  {{/* NOTE: If 's3_aws_access_key_id' and/or 's3_aws_secret_access_key' values
  are not specified, they may instead be provided by the AWS CLI or via the
  respective 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' environment
  variables. */}}
gpudb_conf_diskcache.tmpl: |-
  tier.disk0.default.path = {{ .TierDisk0DefaultPath }}
  tier.disk0.default.limit = {{ .DiskCacheSize }}
  tier.disk0.default.high_watermark = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.DiskCacheTier.Default.HighWatermark }}
  tier.disk0.default.low_watermark = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.DiskCacheTier.Default.LowWatermark }}
  tier.disk0.default.store_persistent_objects = {{ .Cluster.Spec.GPUDBCluster.Config.TieredStorage.DiskCacheTier.DefaultStorePersistentObjects }}
gpudb_conf_postgres_proxy.tmpl: |-
  # ==============================================================================
  # Postgres Proxy Server
  # Start an Postgres(TCP) server as a proxy to handle postgres wire protocol messages.
  enable_postgres_proxy = {{ .Enable }}

  # TCP port that the postgres  proxy server will listen on if
  # 'enable_postgres_proxy' is "true".
  postgres_proxy.port = {{ .Port.ContainerPort }}

  # Set min number of server threads to spawn. (default: "2")
  postgres_proxy.min_threads = {{ .MinThreads }}

  # Set max number of server threads to spawn. (default: "64")
  postgres_proxy.max_threads = {{ .MaxThreads }}

  # Set max number of queued server connections. (default: "1")
  postgres_proxy.max_queued_connections = {{ .MaxQueuedConnections }}

  # Set idle connection  timeout in seconds. (default: "1200")
  postgres_proxy.idle_connection_timeout = {{ .IdleConnectionTimeout }}

  # Set to "true" to use SSL; if "true" then 'ssl_key_file' and
  # 'ssl_cert_file' must be provided
  postgres_proxy.ssl = {{ .SSL }}

  # Files containing the SSL private Key and the SSL certificate for.
  # If required, a self signed certificate (expires after 10 years) can be
  # generated via the command:
  #
  ## openssl req -newkey rsa:2048 -new -nodes -x509 \
  ##   -days 3650 -keyout key.pem -out cert.pem
  #
  postgres_proxy.ssl_key_file =
  postgres_proxy.ssl_cert_file =
gpudb_logger_conf.tmpl: |+
  # ---------------------------------------------------------------------------
  # GPUdb compute cluster logger settings.
  #
  # The logger can be configured to control where to log, how much to log,
  # and how long to keep the log files.
  #
  # The log output can be sent to one or more of the appenders shown below.
  # Unrem only the appender(s) you will use, configure them appropriately, and
  # specify them for the 'rootLogger' and/or specific loggers shown in the
  # middle section of this file.
  #
  # The default 'rootLogger' appender is the 'ToConsole' (stdout) logger which the
  # core/bin/gpudb script pipes to a core/logs/gpudb-HOSTNAME-DATE.log file.
  # This logger is convenient for short time runs of a few days to a week,
  # but take note of the size of the log file and free disk space.
  # Typically about 500 bytes is logged per request, joins and multiple step
  # queries may log more.
  #
  # Note that there will always be a small amount of version info printed to the
  # stdout logfile described above as well as any issues with this configuration file.
  # You should always examine the stdout logfile as it may contain important warnings
  # printed by the system when starting or errors about library paths, for example.
  #
  # ---------------------------------------------------------------------------
  # Log Levels
  #
  # Logger levels are chained together from the bottom 'rootLogger' by the each '.'
  # separator in the logger name. All unspecified loggers use the next lower logger.
  # E.G. 'log4cplus.logger.A.B.C=NOTSET' or not specified at all. Log statements using
  # the A.B.C logger are logged at A.B if set, A if set, or the 'rootLogger' log level.
  #
  # OFF    : Do not log at all.
  # FATAL  : An unrecoverable error has has occurred and the database or component will exit.
  # ERROR  : A serious and unexpected condition has occurred that may severely
  #          impair the database. There should be no ERROR logs when configured correctly.
  #          Ex. Unable to allocate memory, disk full, network errors...
  # WARN   : An unfavorable condition or state has occurred that may impair the database.
  #          Ex. A network outage, reconnect, and resync, slow disks...
  # UERR   : User request is malformed or cannot be processed.
  #          Ex. Invalid table names, bad options...
  # INFO   : Informational messages logged during startup and shutdown.
  #          Once running there are and two INFO logs per request by the head node,
  #          when received and when completed.
  # DEBUG  : Verbose messages about program processing and state.
  # TRACE  : Even more verbose messages about program processing and state.
  # NOTSET : Use the next higher log level in the chained loggers up to 'log4cplus.rootLogger'.
  #          This is equivalent to remming the logger config line or misspelling the log level.
  #
  # ---------------------------------------------------------------------------

  # Kinetica DB version this file is written for.
  file_version = 7.1.1.1.20201104123358.ga

  # Specify the default log level and a comma separated list of appenders to
  # send the log data to.

  log4cplus.rootLogger=INFO, ToConsole, ToSysLog

  # ---------------------------------------------------------------------------
  # Appenders: ToConsole, ToSysLog, ToFile, ToRollingFile, ToRollingDatedFile, ToBacktraceFile
  # ---------------------------------------------------------------------------

  # Below are some preconfigured appenders that can be used to redirect the log output.
  # Unrem and configure the appender(s) as appropriate and use the appender by
  # specifying it in the 'log4cplus.rootLogger' setting above.
  # Note that all unremmed appenders create their output files.

  # ---------------------------------------------------------------------------
  # Console appender logs to stdout.
  #
  # The core/bin/gpudb script pipes to a core/logs/gpudb-HOSTNAME-DATE.log file.
  #

  log4cplus.appender.ToConsole=log4cplus::ConsoleAppender
  log4cplus.appender.ToConsole.layout=log4cplus::PatternLayout
  log4cplus.appender.ToConsole.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n


  # ---------------------------------------------------------------------------
  # Syslog appender
  #
  # Log ouput is sent to the "syslogHost" syslog.
  #

  #log4cplus.appender.ToSysLog=log4cplus::SysLogAppender
  #log4cplus.appender.ToSysLog.layout=log4cplus::PatternLayout
  #log4cplus.appender.ToSysLog.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n
  #log4cplus.appender.ToSysLog.syslogHost=localhost
  #log4cplus.appender.ToSysLog.Facility=user
  #log4cplus.appender.ToSysLog.Threshold=WARN

  log4cplus.appender.ToSysLog=log4cplus::SysLogAppender
  log4cplus.appender.ToSysLog.layout=log4cplus::PatternLayout
  log4cplus.appender.ToSysLog.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %X{JobId}%m%n
  log4cplus.appender.ToSysLog.host=otel-collector.kinetica-system.svc.cluster.local
  log4cplus.appender.ToSysLog.udp=false
  log4cplus.appender.ToSysLog.port=9601
  log4cplus.appender.ToSysLog.Facility=user
  log4cplus.appender.ToSysLog.Threshold=INFO
  log4cplus.appender.ToSysLog.ident=gpudb

  # ---------------------------------------------------------------------------
  # File appender
  #
  # Each processes logs to a single file.
  # This logger is convenient for a short-time use case, take note of log file
  # size and free disk space.
  #
  # Log files will have "-rN" with the rank number or "-h0" for the host manager
  # appended to the name before the extension for each process.
  # A single log file can created by setting UseLockFile=true, see below.
  #

  #log4cplus.appender.ToFile=log4cplus::FileAppender
  #log4cplus.appender.ToFile.File=../logs/gpudb-cluster.log
  #log4cplus.appender.ToFile.Threshold=TRACE
  #log4cplus.appender.ToFile.Append=true
  #log4cplus.appender.ToFile.ImmediateFlush=true
  #log4cplus.appender.ToFile.layout=log4cplus::PatternLayout
  #log4cplus.appender.ToFile.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n

  # Enable the use of a logger lockfile to create a single log file per host.
  # Note that there is a performance penalty for checking the lock file and
  # using separate log files per process is recommended.
  #log4cplus.appender.ToFile.UseLockFile=false

  # ---------------------------------------------------------------------------
  # Rolling log file appender.
  #
  # Each processes logs to a rolling log file set where the "File" name given
  # to the appender is the current log file and the older files are renamed
  # to "File.1", "File.2", ... up to "MaxBackupIndex".
  #
  # Log files will have "-rN" with the rank number or "-hm" for the host manager
  # appended to the name for each process.
  # A single rolling log file can created by setting UseLockFile=true, see below.
  #

  #log4cplus.appender.ToRollingFile=log4cplus::RollingFileAppender
  #log4cplus.appender.ToRollingFile.File=../logs/gpudb-rolling.log
  #log4cplus.appender.ToRollingFile.Threshold=TRACE
  #log4cplus.appender.ToRollingFile.MaxFileSize=200 MB
  #log4cplus.appender.ToRollingFile.MaxBackupIndex=4
  #log4cplus.appender.ToRollingFile.Append=true
  #log4cplus.appender.ToRollingFile.ImmediateFlush=true
  #log4cplus.appender.ToRollingFile.layout=log4cplus::PatternLayout
  #log4cplus.appender.ToRollingFile.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n

  # Enable the use of a logger lockfile to create a single rolling log file per host.
  # Note that there is a performance penalty for checking the lock file and
  # using separate log files per process is recommended.
  #log4cplus.appender.ToRollingFile.UseLockFile=false

  # ---------------------------------------------------------------------------
  # Rolling dated log file appender.
  #
  # Each process logs to a rolling log that is named with the time the file is
  # created and the count of the number of new log files created since started.
  # There will only be "MaxBackupIndex" files kept.
  #
  # Log files will have "-rN" with the rank number or "-hm" for the host manager
  # appended to the name for each process.
  # A single rolling log file can created by setting UseLockFile=true, see below.
  #
  # Example file for rank 0, the head node: 'gpudb-rolling-r0-2017-12-19_03.22.54-2.log'
  # This file would be the 2nd rolling log file since the database was started.
  #

  #log4cplus.appender.ToRollingDatedFile=log4cplus::RollingDatedFileAppender
  #log4cplus.appender.ToRollingDatedFile.File=../logs/gpudb-rolling.log
  #log4cplus.appender.ToRollingDatedFile.Threshold=TRACE
  #log4cplus.appender.ToRollingDatedFile.MaxFileSize=200 MB
  #log4cplus.appender.ToRollingDatedFile.MaxBackupIndex=4
  #log4cplus.appender.ToRollingDatedFile.Append=true
  #log4cplus.appender.ToRollingDatedFile.ImmediateFlush=true
  #log4cplus.appender.ToRollingDatedFile.layout=log4cplus::PatternLayout
  #log4cplus.appender.ToRollingDatedFile.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n

  # Enable the use of a logger lockfile to create a single rolling log file per host.
  # Note that there is a performance penalty for checking the lock file and
  # using separate log files per process is recommended.
  #log4cplus.appender.ToRollingDatedFile.UseLockFile=false

  # ---------------------------------------------------------------------------
  # Backtrace log file appender
  #
  # The 'ToBacktraceFile' appender can be used to redirect the backtrace logs
  # generated by the 'ErrorBacktrace' logger so they will not appear in the
  # 'log4cplus.rootLogger' output.
  # Set 'log4cplus.ErrorBacktrace=ERROR, ToBacktraceFile' in the section below to
  # enable generation of backtraces from certain ERROR logs to this file appender.
  #
  # If using this appender, it is advised to also add this appender to the
  # 'log4cplus.rootLogger' above so that logs at 'Threshold=ERROR' are also logged
  # to give context to the backtrace.
  #

  #log4cplus.appender.ToBacktraceFile=log4cplus::FileAppender
  #log4cplus.appender.ToBacktraceFile.File=../logs/gpudb-backtrace.log
  #log4cplus.appender.ToBacktraceFile.Threshold=ERROR
  #log4cplus.appender.ToBacktraceFile.Append=true
  #log4cplus.appender.ToBacktraceFile.ImmediateFlush=true
  #log4cplus.appender.ToBacktraceFile.layout=log4cplus::PatternLayout
  #log4cplus.appender.ToBacktraceFile.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h %F:%L - %m%n

  # If using an alternate 'ErrorBacktrace' appender set the 'additivity' of the
  # 'ErrorBacktrace' logger to 'false' to avoid duplicating the backtrace logs
  # in the 'rootLogger' output.
  #log4cplus.additivity.ErrorBacktrace=false


  # ===========================================================================
  # Loggers
  #
  # Each logger can have their own log level set and output to one or mode appenders.
  # If the logger or appenders are not specified the 'rootLogger' level
  # or appender(s) are used.
  #
  # Log Levels: NOTSET, TRACE, DEBUG, INFO, UERR, WARN, ERROR, FATAL, OFF
  #
  # Ex. log4cplus.logger.LOGGERNAME=INFO [, optional appender [, appender2]]
  # NOTE: Do not put a '# comment' at the end of the logger.name=level.
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # System and process information loggers
  # ---------------------------------------------------------------------------

  # Log memory info, logs at DEBUG.
  log4cplus.logger.MEMORY=OFF

  # Log numa info for threads, logs at INFO.
  log4cplus.logger.NUMA=INFO

  # Log timing info during processing, logs at DEBUG.
  log4cplus.logger.TIMING=OFF

  # Log nvtx tracing, logs at DEBUG.
  log4cplus.logger.Tracer=OFF

  # Temporarily turn off Postgres logging
  log4cplus.logger.PostgresConnectionHandler=OFF
  log4cplus.logger.BufferedSocketReader=OFF

  # ---------------------------------------------------------------------------
  # GlobalManager / Registry loggers
  # ---------------------------------------------------------------------------

  log4cplus.logger.GlobalManager=INFO
  log4cplus.logger.GlobalManager.pub=INFO
  log4cplus.logger.GlobalManager.pub.socket=INFO
  log4cplus.logger.GlobalManager.recv=INFO
  log4cplus.logger.GlobalManager.recv.socket=INFO

  log4cplus.logger.GlobalRegistry=INFO
  log4cplus.logger.GlobalRegistry.sub=INFO
  log4cplus.logger.GlobalRegistry.sub.socket=INFO


  # ---------------------------------------------------------------------------
  # All rank loggers
  # ---------------------------------------------------------------------------

  #log4cplus.logger.default=INFO

  log4cplus.logger.AvroUtils=INFO

  # Set to ERROR to print a backtrace for certain error logs,
  # else set to OFF to disable backtraces.
  # See ToBacktraceFile appender above for other options.
  log4cplus.logger.ErrorBacktrace=OFF

  log4cplus.logger.ExeProcess=INFO

  log4cplus.logger.File=INFO

  log4cplus.logger.FileSync=INFO

  log4cplus.logger.FilterPlanner=INFO

  log4cplus.logger.FilterPlannerT0=INFO

  log4cplus.logger.GaiaCudaUtils=INFO

  log4cplus.logger.JobManager=INFO

  log4cplus.logger.MemoryMappedFile=INFO

  log4cplus.logger.Message=INFO

  log4cplus.logger.MPI=INFO

  log4cplus.logger.Network=INFO

  log4cplus.logger.OmpManager=INFO

  log4cplus.logger.OmpThread=INFO

  log4cplus.logger.ParserUtils=INFO

  log4cplus.logger.ProcManager=INFO

  log4cplus.logger.SecurityManager=INFO

  log4cplus.logger.StatisticsManager=INFO

  log4cplus.logger.SymbolManager=INFO

  log4cplus.logger.Time=INFO

  log4cplus.logger.Zmq=INFO

  # ---------------------------------------------------------------------------
  # Rank 0 head loggers
  # ---------------------------------------------------------------------------

  log4cplus.logger.Chart=INFO

  log4cplus.logger.DebugRequests=INFO

  log4cplus.logger.DebugRequestErrors=INFO

  log4cplus.logger.DebugResponses=INFO

  log4cplus.logger.EndpointHandler=INFO

  log4cplus.logger.EndpointManager=INFO

  log4cplus.logger.EndpointWorker=INFO

  log4cplus.logger.Graph.Client=INFO

  log4cplus.logger.Graph.Messages=INFO

  log4cplus.logger.HttpRequestHandler=INFO

  # Reduce to DEBUG to show verbose HTTP server connection stats
  log4cplus.logger.HttpRequestHandlerFactory=INFO

  log4cplus.logger.SetMonitor=INFO

  log4cplus.logger.SetManager=INFO

  log4cplus.logger.SetTimeOutManager=INFO

  log4cplus.logger.Sink=INFO

  log4cplus.logger.ShardController=INFO

  log4cplus.logger.StringManager=INFO

  log4cplus.logger.TriggerManager=INFO

  log4cplus.logger.TriggerNotifier=INFO

  log4cplus.logger.WaitQueue=INFO

  log4cplus.logger.WMS=INFO


  # ---------------------------------------------------------------------------
  # Rank 1+ worker loggers
  # ---------------------------------------------------------------------------

  log4cplus.logger.Allocator=INFO

  log4cplus.logger.BytesManager=INFO

  log4cplus.logger.ConcurrentSGMap=INFO

  log4cplus.logger.GaiaSet=INFO

  log4cplus.logger.GaiaSetCompression=INFO

  log4cplus.logger.GaiaSetLockChecker=INFO

  log4cplus.logger.GaiaSetManager=INFO

  log4cplus.logger.GaiaStencil=INFO

  log4cplus.logger.GPUCache=INFO

  log4cplus.logger.GPUManager=INFO

  log4cplus.logger.IndexedDB=INFO

  log4cplus.logger.Kernel=INFO

  log4cplus.logger.Locker=INFO

  log4cplus.logger.Lucene=INFO

  log4cplus.logger.Manager=INFO

  log4cplus.logger.PagedGroupMap=INFO

  log4cplus.logger.Queue=INFO

  log4cplus.logger.ResourceMgmt=INFO

  log4cplus.logger.StringDB=INFO

  log4cplus.logger.TaskReceiver=INFO

  log4cplus.logger.TaskSender=INFO

  log4cplus.logger.TempAllocator=INFO

  log4cplus.logger.Type=INFO

  log4cplus.logger.TypeObjectManager=INFO

  log4cplus.logger.Worker=INFO

  log4cplus.logger.WorkItem=INFO

  log4cplus.logger.UserAuth=INFO

  # ---------------------------------------------------------------------------
  # HA
  # ---------------------------------------------------------------------------

  log4cplus.logger.Ha=INFO

  log4cplus.logger.Ha.ChannelPool=INFO

  log4cplus.logger.Ha.Client=INFO

  log4cplus.logger.Ha.Connection=INFO

  log4cplus.logger.Ha.Handler=INFO

  log4cplus.logger.Ha.Manager=INFO

  log4cplus.logger.Ha.Receiver=INFO

  log4cplus.logger.Ha.WorkItem=INFO

  log4cplus.logger.Ha.WorkPool=INFO

  # ---------------------------------------------------------------------------
  # Etcd
  # ---------------------------------------------------------------------------

  log4cplus.logger.Etcd.Client=INFO

  log4cplus.logger.Etcd.Watcher=INFO

  # ---------------------------------------------------------------------------
  # Auditing
  # ---------------------------------------------------------------------------

  # This section controls the location of the output of the request auditor,
  # which is configured in gpudb.conf in the Auditing section. IMPORTANT:
  # Unless auditing is enabled there, changing these settings will have no
  # effect.

  # All audit messages are at the INFO or ERROR level (ERROR in the event that
  # something prevents full auditing, such as an authentication failure or a
  # badly-formed request). In most cases, the log level here should be left
  # at INFO to avoid filtering any relevant information.

  # The auditor also uses its own appender to separate audit messages from
  # other log messages. This appender can be customized below, and/or other
  # appenders can be added as required here (comma-separated at the end of
  # the line).

  log4cplus.logger.Audit=INFO, AuditAppender

  # The following line prevents audited information from being duplicated in
  # both the audit and regular logs. In most cases, it should not be changed.

  log4cplus.additivity.Audit=false

  # By default, the audit appender will output to the console intermixed with
  # the regular log. This can be customized by changing the following lines.
  # Some example alternative configurations are provided below.

  log4cplus.appender.AuditAppender=log4cplus::ConsoleAppender
  log4cplus.appender.AuditAppender.layout=log4cplus::PatternLayout
  log4cplus.appender.AuditAppender.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h - %m

  # The following example configuration will output to a file instead of the
  # console.

  #log4cplus.appender.AuditAppender=log4cplus::FileAppender
  #log4cplus.appender.AuditAppender.File=gpudb-audit.log
  #log4cplus.appender.AuditAppender.layout=log4cplus::PatternLayout
  #log4cplus.appender.AuditAppender.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h - %m

  # The following example configuration will output to a rolling file instead
  # of the console.

  #log4cplus.appender.AuditAppender=log4cplus::RollingFileAppender
  #log4cplus.appender.AuditAppender.File=../logs/gpudb-audit.log
  #log4cplus.appender.AuditAppender.MaxFileSize=20 MB
  #log4cplus.appender.AuditAppender.MaxBackupIndex=4
  #log4cplus.appender.AuditAppender.Append=false
  #log4cplus.appender.AuditAppender.layout=log4cplus::PatternLayout
  #log4cplus.appender.AuditAppender.layout.ConversionPattern=%D{%Y-%m-%d %H:%M:%S.%q} %-5p (%i,%T,%-18t) %h - %m

httpd_conf.tmpl: "# ---------------------------------------------------------------------------\n
  # Kinetica Apache httpd configuration for virtual hosts requiring\n# authorization.\
  \  This virtual host should authenticate the user in whatever\n# means is desired
  and then set a REMOTE_USER header on the request so that\n# Kinetica will know what
  user is connected.\n#\n# Created by the Kinetica DB Operator.\n#\n# ---------------------------------------------------------------------------\n
  # fix for the error (95)Operation not supported: AH00023: Couldn't create the ssl-cache
  mutex \nMutex posixsem\n#httpd: Could not reliably determine the server's fully
  qualified domain name, using 10.244.1.9. Set the 'ServerName' directive globally
  to suppress this message\nServerName \"gpudb\"\n# TCP port for httpd server to listen
  on.\n# Note that this port value is overwritten by the port specified in the\n#
  gpudb.conf's 'httpd_proxy_port' parameter at startup.\nListen 8082\n<VirtualHost
  *:8082>\n    # ---------------------------------------------------------------------------\n\
  \    # Logging\n    ErrorLog \"|/opt/gpudb/httpd/bin/rotatelogs -n 2 logs/error_log
  100M\"\n    # LogLevel error\n    CustomLog \"|/opt/gpudb/httpd/bin/rotatelogs -n
  2 logs/access_log 100M\" common\n\n    KeepAlive Off\n    # KeepAliveTimeout 5\n\
  \    # MaxKeepAliveRequests 0\n\n    # Default to about a 1 month timeout - let
  GPUdb handle the timeout internally\n    TimeOut 2500000\n\n    <IfModule log_config_module>\n\
  \        LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\
  \"\" combined\n        LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b\" common\n\n  \
  \      <IfModule logio_module>\n            LogFormat \"%h %l %u %t \\\"%r\\\" %>s
  %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %I %O\" combinedio\n        </IfModule>\n\
  \n        CustomLog \"logs/access_log\" common\n    </IfModule>\n\n    # ---------------------------------------------------------------------------\n\
  \    # Use LDAP auth for all URLs \"/g*\"\n    # A /gpudb-ldap.html is available
  for testing the auth credentials.\n    #\n    # Configure the LDAP server variables
  as needed.\n\n    <Location />\n        Header set Access-Control-Allow-Origin \"\
  *\"\n\n        # Clear the REMOTE_USER and KINETICA_ROLES headers, so that they\n\
  \        # cannot be passed through by the client.\n        Header unset REMOTE_USER\n\
  \        Header unset KINETICA_ROLES\n\n        AuthName \"Kinetica Authentication\"\
  \n        AuthType Basic\n        AuthBasicProvider ldap\n        AuthLDAPUrl {{
  .URL }}\n        AuthLDAPBindDN {{ .BindDN }}\n        AuthLDAPBindPassword {{ .Auth.AdminPassword
  }}\n        AuthBasicAllowAnonymous on\n\n        <LimitExcept OPTIONS>\n      \
  \      <RequireAll>\n#                Block commented as it disallows none LDAP
  internal users e.g. kifs from accessing the APIs\n#                <RequireAny>\n\
  #                    Require ldap-group cn=db_users,ou=Groups, dc=kinetica,dc=com\n\
  #                    Require ldap-group cn=global_admins,ou=Groups, dc=kinetica,dc=com\n\
  #                </RequireAny>\n                <RequireAny>\n                 \
  \   <RequireAny>\n                        Require kinetica-ldap-role-mapping global_admins
  cn=global_admins,ou=Groups,dc=kinetica,dc=com\n                        Require kinetica-ldap-role-mapping
  db_users cn=db_users,ou=Groups,dc=kinetica,dc=com\n                    </RequireAny>\n\
  \                    Require valid-user\n                </RequireAny>\n       \
  \     </RequireAll>\n        </LimitExcept>\n\n        ## Add REMOTE_USER, KINETICA_IMPERSONATE
  and KINETICA_ROLES HTTP headers,\n        ## and do not pass through any AUTHORIZATION
  header containing LDAP credentials.\n        ## Update the first two lines below
  if an attribute other than \"uid\" is used for\n        ## the user ID.\n      \
  \  RequestHeader set REMOTE_USER %{AUTHENTICATE_uid}e env=AUTHENTICATE_uid\n   \
  \     # RequestHeader set KINETICA_IMPERSONATE %{IMPERSONATE_uid}e env=IMPERSONATE_uid\n\
  \        RequestHeader set KINETICA_ROLES %{KINETICA_ROLES}e env=KINETICA_ROLES\n\
  \        RequestHeader unset AUTHORIZATION env=AUTHENTICATE_uid\n    </Location>\n\
  \n\n    # Info about ldap caching at \"/server/ldap-info\"\n    <Location \"/server/ldap-info\"\
  >\n        SetHandler ldap-status\n    </Location>\n\n    ProxyPreserveHost On\n\
  \n    # The placeholder (COMPONENTCONFIG) below is replaced when\n    # /opt/gpudb/core/bin/gpudb
  copies httpd.conf to httpd_gpudb.conf and adds\n    # the correct ProxyPass lines
  below.\n    #\n    # httpd urls /gpudb-N -> http://gpudb_server_rank_N:port\n  \
  \  #\n    # The generated commands should look something like this.\n    #\n   \
  \ #   ProxyPass        /gpudb-0 http://127.0.0.1:9191/\n    #   ProxyPassReverse
  /gpudb-0 http://127.0.0.1:9191/\n    #   ProxyPass        /gpudb-1 http://127.0.0.1:9192/\n\
  \    #   ProxyPassReverse /gpudb-1 http://127.0.0.1:9192/\n    #   ...\n\n##COMPONENTCONFIG##\n\
  </VirtualHost>\n\n"
input-gpudb-fluent-bit.conf.tmpl: |-
  [INPUT]
      Name           tcp
      Listen         0.0.0.0
      Port           5170
      Chunk_Size     32
      Buffer_Size    64
      Format         json
      Mem_Buf_Limit  5MB
kml_ini.tmpl: |
  # ==============================================================================
  # Kinetica ML server configuration file
  # ==============================================================================

  [general]

  # Set to 'False' to disable the auto-configuration of the network settings
  # by bin/kml at startup to accomodate special networks. Default is 'True'.
  auto_configure_connections=True

  [api]
  # ==============================================================================
  # Workbench API general settings

  # Internal/Local network connection to the workbench API. (http://<local_ip>:9187)
  # Automatically configured by bin/kml at start.
  api_connection=
  # External network connection to the workbench API. (http://<public_ip>:9187)
  # Automatically configured by bin/kml at start.
  api_public_connection=

  # App server debug mode.
  # Setting True will also limit API to single thread and enable
  # auto-restart on change.
  server_debug=False

  # Numbers of API pre-fork worker threads for to use.
  n_workers=10

  [database]
  # ==============================================================================
  # Database connection settings

  # Internal/Local network URL to the GPUdb database. (http://<gpudb_ip>:9191)
  # Automatically configured by bin/kml at start.
  db_connection=
  # External network URL to the GPUdb database.
  # Automatically configured by bin/kml at start.
  db_public_connection=

  # Internal/Local network connection to the GPUdb table monitor publisher. (tcp://<gpudb_ip>:9002)
  # Automatically configured by bin/kml at start.
  table_monitor_connection=
  # External network connection to the GPUdb table monitor publisher.
  # Automatically configured by bin/kml at start.
  table_monitor_public_connection=

  # Database credentials.
  # Leave blank if DB auth is disabled, otherwise use 'kpass' utility to
  # populate these fields with encrypted credentials.
  user= {{ .User }}
  pass= {{ .Password }}

  # Database schemas for AAW system tables, audit tables and archives.
  schema_sys=kml
  schema_audit=kml_audit
  schema_sys_archived=kml_archive
  schema_audit_archived=kml_audit_archive

  [cluster]
  # ==============================================================================
  # Kubernetes cluster connection settings

  # Kubernetes config file (kubeconfig) location.
  # If blank, the default location of /opt/gpudb/.kube/config is used.
  # Otherwise, specify with full path.
  kube_config=

  # Kubernetes namespace for this API instance.
  # Customizing this namespace allows for compute resouce sharing by allowing
  # cluster multi-tenancy. If not included, the "kml-" prefix will be added.
  k8s_namespace=kml-active-analytics

  # Worker and BYOC image pull policy.
  # If False, the user facing containers (BYOC and Blackbox model worker) will
  # only be pulled if not present in the K8s docker cache (default). If set to
  # True, these containers will always be pulled.
  always_pull_user_images=False

  # Pod resource provisioning defaults (Memory).
  # Memory requests are made in mebibytes (Mi) units. Where,
  # 1024Mi = 1 gibibyte addressable RAM on the K8s cluster. The default
  # request must be <= to the limit.
  mem_request = 512
  mem_limit = 2048

  # Pod resource provisioning defaults (GPU).
  # NVIDIA CUDA capable GPU requests are integer values. Fractional
  # GPU allocation is not allowed. For GPGPU provisioning, the request is
  # equivalent to the limit.
  gpu_request = 2

  # Pod resource provisioning defaults (CPU).
  # CPU requests are made in mili-cpu (m) units as shown. Where,
  # 1000m = 1 cloud vCPU or 1 logical core (hyperthread) of a physical core.
  # The default request must be <= to the limit.
  cpu_request = 500
  cpu_limit = 2000

  [response]
  # ==============================================================================
  # API response settings

  # Log level (critical|error|warning|info|debug)
  log_level=error

  # Log select request and response payloads to disk
  enable_profiling=False

  [state_manager]
  # ==============================================================================
  # Configurations for state manager (kmlsweep)

  # Polling interval
  # Typical range for optimal UX feedback: 2 to 60 seconds
  wait_between_checks=5


  # Copyright (c) 2021 Kinetica DB Inc.
  # for support email: support@kinetica.com
odbc_ini.tmpl: |
  [ODBC Data Sources]
  KINETICA=KineticaODBCDriver
  KINETICA_READONLY=KineticaODBCDriver

  [KINETICA]
  Description=Kinetica Connection
  Driver=/opt/gpudb/connectors/odbc/lib/libKineticaODBC.so
  URL=http://localhost:8082/gpudb-0
  UID=
  PWD=
  Timeout=0
  Locale=en-US
  SslCACertPath=/opt/gpudb/certs
  SslAllowHostMismatch=0
  PagingTableTtl=10
  #GroupByForceReplicated=0
  #KeepTempTables=0
  #NoCostOptimizer=0
  #NoDistributedJoins=0
  #NoMaterializedViewAccess=0
  #NoParallel=0
  #NoPlanCache=0
  #NoQueryCache=0
  #NoRuleBasedQueryRewrites=0
  #RowsPerFetch=20000
  #TTL=20


  [READONLY_GPUDB_DSN]
  Description=ReadOnly Kinetica Connection
  Driver=/opt/gpudb/connectors/odbc/lib/libKineticaODBC.so
  URL=http://localhost:8082/gpudb-0
  UID=
  PWD=
  Timeout=0
  Locale=en-US
  SslCACertPath=/opt/gpudb/certs
  SslAllowHostMismatch=1
  ReadOnly=1
output-stdout-fluent-bit.conf.tmpl: |-
  [OUTPUT]
      Name           stdout
      Match          *
